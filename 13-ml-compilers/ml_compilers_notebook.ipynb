{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Compilers: Interactive Learning Notebook\n",
    "\n",
    "This notebook provides hands-on experiments with ML compilers.\n",
    "Run each cell to understand what compilers do and why they matter.\n",
    "\n",
    "## Topics Covered\n",
    "1. Why Compilers Matter - Eager vs Compiled\n",
    "2. PyTorch torch.compile\n",
    "3. Kernel Fusion Demonstration\n",
    "4. Profiling Compiled Code\n",
    "5. Comparison Across Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Profiling Utility\n",
    "\n",
    "First, let's create a proper profiling function that handles CUDA synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_fn(func, warmup=10, iterations=100, name=\"\"):\n",
    "    \"\"\"Profile a function with proper CUDA timing.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        for _ in range(iterations):\n",
    "            func()\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        return start.elapsed_time(end) / iterations\n",
    "    else:\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            func()\n",
    "        return (time.perf_counter() - start) * 1000 / iterations\n",
    "\n",
    "print(\"Profiling utility ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Compilers Matter: Eager vs Compiled\n",
    "\n",
    "Let's see the difference between eager execution and compiled execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple operations that should be fused\n",
    "size = 10_000_000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "a = torch.randn(size, device=device)\n",
    "b = torch.randn(size, device=device)\n",
    "c = torch.randn(size, device=device)\n",
    "\n",
    "# Eager mode: Each operation is a separate kernel\n",
    "def eager_compute():\n",
    "    x = a + b        # Kernel 1: Read a,b → Write x\n",
    "    y = x * c        # Kernel 2: Read x,c → Write y  \n",
    "    z = torch.relu(y) # Kernel 3: Read y → Write z\n",
    "    return z\n",
    "\n",
    "time_eager = profile_fn(eager_compute)\n",
    "print(f\"Eager mode: {time_eager:.3f} ms\")\n",
    "print(f\"Memory traffic: 6 passes (read a,b,write x, read x,c,write y, read y,write z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiled mode: Operations are fused into one kernel\n",
    "try:\n",
    "    compiled_compute = torch.compile(eager_compute)\n",
    "    \n",
    "    # First call triggers compilation\n",
    "    _ = compiled_compute()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    time_compiled = profile_fn(compiled_compute)\n",
    "    \n",
    "    print(f\"Compiled mode: {time_compiled:.3f} ms\")\n",
    "    print(f\"Memory traffic: 2 passes (read a,b,c → write z)\")\n",
    "    print(f\"\\nSpeedup: {time_eager/time_compiled:.2f}x\")\n",
    "    print(f\"\\nWhy faster? Compiler fused 3 kernels into 1!\")\n",
    "except Exception as e:\n",
    "    print(f\"torch.compile not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Compilation\n",
    "\n",
    "Let's understand what torch.compile does under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain what Dynamo captures\n",
    "def simple_fn(x):\n",
    "    y = x.sin()\n",
    "    z = y.cos()\n",
    "    return z + 1\n",
    "\n",
    "try:\n",
    "    x = torch.randn(100, device=device)\n",
    "    explanation = torch._dynamo.explain(simple_fn)(x)\n",
    "    \n",
    "    print(\"TorchDynamo Analysis:\")\n",
    "    print(f\"  Graph breaks: {explanation.graph_break_count}\")\n",
    "    print(f\"  Operations captured: {len(explanation.graphs)} graph(s)\")\n",
    "    print(f\"\\nNo graph breaks = everything can be compiled together!\")\n",
    "except Exception as e:\n",
    "    print(f\"Dynamo explain not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GELU Fusion Example\n",
    "\n",
    "GELU is a great example because it involves multiple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 16_000_000\n",
    "x = torch.randn(size, device=device)\n",
    "\n",
    "# Manual GELU: Multiple operations\n",
    "def manual_gelu(x):\n",
    "    # GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x³)))\n",
    "    return 0.5 * x * (1.0 + torch.tanh(0.7978845608 * (x + 0.044715 * x**3)))\n",
    "\n",
    "# PyTorch's fused GELU\n",
    "def pytorch_gelu(x):\n",
    "    return F.gelu(x)\n",
    "\n",
    "# Profile\n",
    "time_manual = profile_fn(lambda: manual_gelu(x))\n",
    "time_pytorch = profile_fn(lambda: pytorch_gelu(x))\n",
    "\n",
    "print(f\"GELU Implementations ({size/1e6:.0f}M elements):\")\n",
    "print(f\"  Manual (unfused):  {time_manual:.3f} ms\")\n",
    "print(f\"  PyTorch (fused):   {time_pytorch:.3f} ms\")\n",
    "print(f\"  Speedup: {time_manual/time_pytorch:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let torch.compile fuse our manual implementation\n",
    "try:\n",
    "    compiled_gelu = torch.compile(manual_gelu)\n",
    "    _ = compiled_gelu(x)  # Warmup/compile\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    time_compiled = profile_fn(lambda: compiled_gelu(x))\n",
    "    \n",
    "    print(f\"torch.compile on manual GELU: {time_compiled:.3f} ms\")\n",
    "    print(f\"Speedup vs manual: {time_manual/time_compiled:.2f}x\")\n",
    "    print(f\"\\nCompiler automatically fused all the operations!\")\n",
    "except Exception as e:\n",
    "    print(f\"Compilation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Compilation\n",
    "\n",
    "Let's compile an entire model and see the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple transformer-like block\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ln(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Setup\n",
    "batch, seq, dim = 32, 512, 768\n",
    "model = SimpleBlock(dim).to(device)\n",
    "x = torch.randn(batch, seq, dim, device=device)\n",
    "\n",
    "# Eager\n",
    "time_eager = profile_fn(lambda: model(x), iterations=50)\n",
    "print(f\"Model input: ({batch}, {seq}, {dim})\")\n",
    "print(f\"Eager mode: {time_eager:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with different modes\n",
    "results = [(\"Eager\", time_eager)]\n",
    "\n",
    "modes = [\n",
    "    (\"default\", {}),\n",
    "    (\"reduce-overhead\", {\"mode\": \"reduce-overhead\"}),\n",
    "]\n",
    "\n",
    "for mode_name, kwargs in modes:\n",
    "    try:\n",
    "        compiled_model = torch.compile(model, **kwargs)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            _ = compiled_model(x)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        time_compiled = profile_fn(lambda: compiled_model(x), iterations=50)\n",
    "        results.append((f\"compile ({mode_name})\", time_compiled))\n",
    "    except Exception as e:\n",
    "        print(f\"Mode {mode_name} failed: {e}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'Mode':<30} {'Time (ms)':<15} {'Speedup'}\")\n",
    "print(\"-\" * 55)\n",
    "base_time = results[0][1]\n",
    "for name, t in results:\n",
    "    print(f\"{name:<30} {t:<15.3f} {base_time/t:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mixed Precision Impact\n",
    "\n",
    "Compilers also optimize for different precisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiply at different precisions\n",
    "N = 4096\n",
    "\n",
    "# FP32\n",
    "A_fp32 = torch.randn(N, N, device=device)\n",
    "B_fp32 = torch.randn(N, N, device=device)\n",
    "\n",
    "# FP16\n",
    "A_fp16 = A_fp32.half()\n",
    "B_fp16 = B_fp32.half()\n",
    "\n",
    "# Profile\n",
    "time_fp32 = profile_fn(lambda: A_fp32 @ B_fp32, iterations=50)\n",
    "time_fp16 = profile_fn(lambda: A_fp16 @ B_fp16, iterations=50)\n",
    "\n",
    "# Calculate TFLOPS\n",
    "flops = 2 * N * N * N\n",
    "tflops_fp32 = flops / (time_fp32 / 1000) / 1e12\n",
    "tflops_fp16 = flops / (time_fp16 / 1000) / 1e12\n",
    "\n",
    "print(f\"Matrix multiply {N}x{N}:\")\n",
    "print(f\"  FP32: {time_fp32:.3f} ms ({tflops_fp32:.1f} TFLOPS)\")\n",
    "print(f\"  FP16: {time_fp16:.3f} ms ({tflops_fp16:.1f} TFLOPS)\")\n",
    "print(f\"  Speedup: {time_fp32/time_fp16:.2f}x\")\n",
    "print(f\"\\nFP16 uses Tensor Cores for massive speedup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory-Bound vs Compute-Bound\n",
    "\n",
    "Understanding this distinction is key to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4096\n",
    "\n",
    "# Element-wise operation (memory-bound)\n",
    "x = torch.randn(N * N, device=device)\n",
    "time_add = profile_fn(lambda: x + 1.0)\n",
    "\n",
    "# Arithmetic intensity = 1 FLOP / 8 bytes = 0.125\n",
    "print(\"Element-wise add (memory-bound):\")\n",
    "print(f\"  Time: {time_add:.3f} ms\")\n",
    "print(f\"  Arithmetic intensity: ~0.125 FLOP/byte\")\n",
    "\n",
    "# Matrix multiply (compute-bound)\n",
    "A = torch.randn(N, N, device=device)\n",
    "B = torch.randn(N, N, device=device)\n",
    "time_mm = profile_fn(lambda: A @ B, iterations=50)\n",
    "\n",
    "# Arithmetic intensity = 2N FLOPs / 8 bytes ≈ 1024 for N=4096\n",
    "print(f\"\\nMatrix multiply (compute-bound):\")\n",
    "print(f\"  Time: {time_mm:.3f} ms\")\n",
    "print(f\"  Arithmetic intensity: ~{2*N/8:.0f} FLOP/byte\")\n",
    "\n",
    "print(f\"\\nKey insight:\")\n",
    "print(f\"  Memory-bound ops benefit from FUSION (less memory traffic)\")\n",
    "print(f\"  Compute-bound ops benefit from Tensor Cores (more compute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Profiling with PyTorch Profiler\n",
    "\n",
    "Let's see what kernels are actually being run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Profile eager execution\n",
    "model = SimpleBlock(512).to(device)\n",
    "x = torch.randn(16, 256, 512, device=device)\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    for _ in range(5):\n",
    "        _ = model(x)\n",
    "\n",
    "print(\"Eager mode kernel breakdown:\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile compiled execution\n",
    "try:\n",
    "    compiled_model = torch.compile(model)\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        _ = compiled_model(x)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True\n",
    "    ) as prof:\n",
    "        for _ in range(5):\n",
    "            _ = compiled_model(x)\n",
    "    \n",
    "    print(\"Compiled mode kernel breakdown:\")\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    print(\"\\nNotice: Triton kernels appear, operations are fused!\")\n",
    "except Exception as e:\n",
    "    print(f\"Compilation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Key takeaways from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    ML COMPILERS SUMMARY                          ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  1. KERNEL FUSION is the #1 optimization                        ║\n",
    "║     • Reduces memory traffic                                    ║\n",
    "║     • Most ML ops are memory-bound                              ║\n",
    "║                                                                  ║\n",
    "║  2. torch.compile is easy to use                                ║\n",
    "║     • Just wrap your model: torch.compile(model)                ║\n",
    "║     • Works with most PyTorch code                              ║\n",
    "║                                                                  ║\n",
    "║  3. PROFILING is essential                                      ║\n",
    "║     • Always sync CUDA before timing                            ║\n",
    "║     • Use PyTorch profiler for details                          ║\n",
    "║                                                                  ║\n",
    "║  4. Different compilers for different use cases                 ║\n",
    "║     • Training: torch.compile                                   ║\n",
    "║     • NVIDIA inference: TensorRT                                ║\n",
    "║     • TPU: JAX/XLA                                              ║\n",
    "║     • Edge: TVM, ONNX Runtime                                   ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
