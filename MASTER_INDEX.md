# Master Index: Deep Foundations for ML Systems

Complete learning materials for understanding ML systems from first principles.

## ğŸ“š Module Overview

| # | Module | Focus | Files |
|---|--------|-------|-------|
| 00 | Crucial Concepts | Often-ignored fundamentals | 3+ |
| 01 | Computer Architecture | CPU, cache, SIMD | 15+ |
| 02 | Operating Systems | mmap, processes, I/O | 15+ |
| 03 | Assembly Programming | x86-64, AVX, optimization | 13+ |
| 04 | C Programming | Pointers, memory, patterns | 17+ |
| 05 | C++ Programming | RAII, smart pointers, move | 4+ |
| 06 | Hardware Fundamentals | PCIe, latency, bandwidth | 5+ |
| 07 | Multimodal Data Formats | Text, image, audio, video | 8+ |
| 08 | Device I/O | Data acquisition, DMA | 2+ |
| 10 | GPU Architecture | Memory hierarchy, Tensor Cores | 5+ |
| 11 | CUDA Programming | Kernels, optimization | 6+ |
| 12 | Triton Programming | **Unsloth kernels, quantization** | 10+ |
| 13 | ML Compilers | TorchDynamo, Inductor | 6+ |
| 14 | PyTorch Internals | Dispatcher, autograd | 4+ |
| 15 | Attention Mechanisms | Flash Attention, efficient attention | 7+ |
| 16 | Training Optimization | **LoRA, quantization, fusion** | 10+ |

## ğŸ¯ Learning Paths

### Path 1: Systems Foundations (4 weeks)
```
Week 1: 01-computer-architecture (binary, cache, SIMD)
Week 2: 02-operating-systems (mmap, processes)
Week 3: 04-c-programming (pointers, memory)
Week 4: 06-hardware-fundamentals (latency, PCIe)
```

### Path 2: Low-Level Optimization (4 weeks)
```
Week 1: 03-assembly-programming/01-x86-64-basics
Week 2: 03-assembly-programming/02-simd-avx
Week 3: 03-assembly-programming/03-optimization-patterns
Week 4: 00-crucial-concepts (bandwidth, precision)
```

### Path 3: Multimodal Pipeline (3 weeks)
```
Week 1: 07-multimodal-data-formats/01-text-encoding
        07-multimodal-data-formats/02-image-formats
Week 2: 07-multimodal-data-formats/03-audio-formats
        07-multimodal-data-formats/04-video-formats
Week 3: 08-device-io
        07-multimodal-data-formats/05-ml-data-formats
```

### Path 4: C++/Modern Systems (2 weeks)
```
Week 1: 05-cpp-programming/01-memory-management
Week 2: Advanced patterns and integration
```

### Path 5: GPU & Triton Mastery (6 weeks) â˜… NEW
```
Week 1: 10-gpu-architecture (memory hierarchy, Tensor Cores)
Week 2: 11-cuda-programming (basics, optimization)
Week 3: 12-triton-programming/basics + puzzles
Week 4: 12-triton-programming/patterns (softmax, matmul)
Week 5: 12-triton-programming/advanced (Flash Attention, Unsloth kernels)
Week 6: 12-triton-programming/advanced (quantization kernels)
```

### Path 6: LLM Training Optimization (4 weeks) â˜… NEW
```
Week 1: 16-training-optimization/mixed-precision + memory
Week 2: 16-training-optimization/fine-tuning (LoRA, QLoRA)
Week 3: 16-training-optimization/fusion + quantization
Week 4: 16-training-optimization/compilation (torch.compile)
```

## ğŸ“ Complete File Listing

### 00-crucial-concepts/ (PROFILING FOCUSED)
```
â”œâ”€â”€ README.md                           # Top 10 ignored concepts
â”œâ”€â”€ 01_memory_bandwidth_bottleneck.md   # #1 performance issue
â”œâ”€â”€ 02_floating_point_precision.md      # Numerical stability
â”œâ”€â”€ 03_profiling_fundamentals.md        # How to profile (ESSENTIAL)
â”œâ”€â”€ 04_data_movement_costs.c            # Data movement benchmarks
â”œâ”€â”€ 05_profiling_multimodal_pipeline.py # Profile image/audio/video
â”œâ”€â”€ 06_cpu_gpu_transfer_costs.py        # CPUâ†”GPU transfer profiling
â””â”€â”€ 07_end_to_end_pipeline_profile.py   # Full training loop profiler
```

### 01-computer-architecture/
```
â”œâ”€â”€ 01-binary-and-bits/
â”œâ”€â”€ 02-memory-hierarchy/
â”œâ”€â”€ 03-simd-vectorization/
â”œâ”€â”€ 04-memory-alignment/
â”œâ”€â”€ 05-cpu-pipeline/
â”œâ”€â”€ 06-data-layout/
â”œâ”€â”€ 07-benchmarking/
â””â”€â”€ exercises/
```

### 02-operating-systems/
```
â”œâ”€â”€ 01-memory-mapping/
â”œâ”€â”€ 02-processes-threads/
â”œâ”€â”€ 03-file-io/
â”œâ”€â”€ 04-virtual-memory/
â”œâ”€â”€ 05-system-calls/
â”œâ”€â”€ 06-memory-allocators/
â”œâ”€â”€ 07-synchronization/
â”œâ”€â”€ 08-shared-memory-ipc/
â””â”€â”€ exercises/
```

### 03-assembly-programming/
```
â”œâ”€â”€ 01-x86-64-basics/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_hello_world.s
â”‚   â”œâ”€â”€ 02_registers.s
â”‚   â””â”€â”€ 02_registers_main.c
â”œâ”€â”€ 02-simd-avx/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_avx_basics.c
â”‚   â”œâ”€â”€ 02_avx_dotproduct.s
â”‚   â””â”€â”€ 02_dotprod_main.c
â”œâ”€â”€ 03-optimization-patterns/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_quantized_dot.c
â”‚   â””â”€â”€ 02_prefetch_patterns.c
â”œâ”€â”€ 04-reading-compiler-output/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ 01_simple_functions.c
â””â”€â”€ LEARNING_ORDER.md
```

### 04-c-programming/
```
â”œâ”€â”€ 01-pointers-deep-dive/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_pointer_basics.c
â”‚   â”œâ”€â”€ 02_pointer_arithmetic.c
â”‚   â”œâ”€â”€ 03_void_pointers.c
â”‚   â””â”€â”€ 04_function_pointers.c
â”œâ”€â”€ 02-memory-management/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_stack_vs_heap.c
â”‚   â””â”€â”€ 02_custom_allocator.c
â”œâ”€â”€ 03-mmap-advanced/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_mmap_file_io.c
â”‚   â””â”€â”€ 02_shared_tensor.c
â”œâ”€â”€ 04-struct-patterns/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ 01_data_oriented.c
â”œâ”€â”€ 05-io-patterns/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ 01_buffered_io.c
â””â”€â”€ 06-ffcv-patterns/
    â””â”€â”€ 01_ffcv_analysis.md
```

### 05-cpp-programming/
```
â”œâ”€â”€ 01-memory-management/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_raii.cpp
â”‚   â”œâ”€â”€ 02_smart_pointers.cpp
â”‚   â””â”€â”€ 03_move_semantics.cpp
â””â”€â”€ README.md
```

### 06-hardware-fundamentals/
```
â”œâ”€â”€ README.md
â”œâ”€â”€ 01-system-architecture/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_latency_numbers.c
â”‚   â””â”€â”€ 02_pcie_deep_dive.md
â”œâ”€â”€ 02-memory-hierarchy-deep/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_cache_line_effects.c
â”‚   â””â”€â”€ 02_bandwidth_profiled.c      # Memory bandwidth profiling
```

### 07-multimodal-data-formats/ (ALL PROFILED)
```
â”œâ”€â”€ README.md
â”œâ”€â”€ 01-text-encoding/
â”‚   â””â”€â”€ 01_unicode_utf8.c
â”œâ”€â”€ 02-image-formats/
â”‚   â”œâ”€â”€ 01_image_fundamentals.md
â”‚   â””â”€â”€ 02_image_decode_profiled.c   # Image pipeline with timing
â”œâ”€â”€ 03-audio-formats/
â”‚   â”œâ”€â”€ 01_audio_fundamentals.md
â”‚   â””â”€â”€ 02_audio_processing_profiled.c # Audio pipeline with timing
â”œâ”€â”€ 04-video-formats/
â”‚   â”œâ”€â”€ 01_video_fundamentals.md
â”‚   â”œâ”€â”€ 02_color_spaces.c
â”‚   â””â”€â”€ 03_video_decode_profiled.c   # Video pipeline with timing
â””â”€â”€ 05-ml-data-formats/
    â”œâ”€â”€ 01_tensor_storage.md
    â””â”€â”€ 02_multimodal_batch_profiled.py # Batch creation profiling
```

### 08-device-io/
```
â””â”€â”€ README.md                        # DMA, ring buffers, interfaces
```

### 09-data-loading-pipelines/
```
â”œâ”€â”€ 01_dataloader_profiling.py       # PyTorch DataLoader profiling
â””â”€â”€ 02_ffcv_webdataset_comparison.md # FFCV vs WebDataset analysis
```

### 12-triton-programming/ â˜… EXTENSIVELY UPDATED
```
â”œâ”€â”€ README.md                        # Overview with Unsloth coverage
â”œâ”€â”€ basics/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ 01_triton_fundamentals.py    # Core concepts, profiling
â”œâ”€â”€ patterns/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_softmax_kernel.py         # Online softmax algorithm
â”‚   â””â”€â”€ 02_matmul_kernel.py          # Tiled matmul, auto-tuning
â”œâ”€â”€ advanced/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_flash_attention.py        # Flash Attention deep dive
â”‚   â”œâ”€â”€ 02_unsloth_kernels.py        # â˜… NEW: Production kernels
â”‚   â”‚   â”œâ”€â”€ Fused RMSNorm + Residual
â”‚   â”‚   â”œâ”€â”€ Fused Cross-Entropy (chunked)
â”‚   â”‚   â”œâ”€â”€ Fused RoPE
â”‚   â”‚   â”œâ”€â”€ Fused SwiGLU
â”‚   â”‚   â””â”€â”€ Fused LoRA forward
â”‚   â”œâ”€â”€ 03_quantization_kernels.py   # â˜… NEW: Quantization
â”‚   â”‚   â”œâ”€â”€ INT8 quantize/dequantize
â”‚   â”‚   â”œâ”€â”€ INT8 matmul with dequant
â”‚   â”‚   â”œâ”€â”€ NF4 (QLoRA) concepts
â”‚   â”‚   â”œâ”€â”€ FP8 (Hopper)
â”‚   â”‚   â””â”€â”€ Dynamic quantization
â”‚   â””â”€â”€ flash-attention/
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ puzzles/                         # â˜… NEW: Practice problems
â”‚   â””â”€â”€ 01_triton_puzzles.py         # 9 exercises with solutions
â””â”€â”€ triton_programming_notebook.ipynb
```

### 16-training-optimization/ (COMPREHENSIVE)
```
â”œâ”€â”€ README.md
â”œâ”€â”€ mixed-precision/
â”‚   â”œâ”€â”€ 01_floating_point_formats.py # FP32, FP16, BF16, FP8
â”‚   â””â”€â”€ 02_automatic_mixed_precision.py
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ 01_gradient_checkpointing.py
â”‚   â””â”€â”€ 02_gradient_accumulation_8bit_optimizers.py
â”œâ”€â”€ fine-tuning/
â”‚   â””â”€â”€ lora/
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ 01_lora_deep_dive.py     # LoRA, QLoRA, DoRA
â”œâ”€â”€ quantization/
â”‚   â””â”€â”€ 01_quantization_fundamentals.py
â”œâ”€â”€ fusion/
â”‚   â””â”€â”€ 01_operator_fusion.py        # Unsloth-style fusion
â””â”€â”€ compilation/
    â””â”€â”€ 01_torch_compile.py          # TorchDynamo, Inductor
```

## ğŸ”‘ Key Concepts by Topic

### Memory & Performance
- Cache lines (64 bytes)
- Memory bandwidth bottleneck
- Data alignment
- False sharing
- NUMA effects

### Data Representation
- UTF-8 encoding
- IEEE 754 floating point
- YUV color spaces
- PCM audio
- Video codecs (H.264, H.265)

### System Interfaces
- mmap for zero-copy
- PCIe for GPU transfer
- DMA for async I/O
- Pinned memory

### Optimization Techniques
- SIMD/AVX vectorization
- Cache blocking/tiling
- Prefetching
- Kernel fusion
- Quantization (INT8/INT4)

### Triton & GPU Programming â˜… NEW
- Block-based programming model
- Auto-tuning configurations
- Memory coalescing patterns
- Fused kernels (RMSNorm, CrossEntropy, RoPE)
- Online softmax algorithm
- Flash Attention tiling

### Unsloth Optimizations â˜… NEW
- Fused RMSNorm + Residual (2-3x speedup)
- Chunked Cross-Entropy (10x memory reduction)
- Fused SwiGLU MLP
- Fused LoRA forward/backward
- NF4/INT8 quantization kernels

### LLM Inference Optimization â˜… NEW
- KV Cache fundamentals and memory analysis
- PagedAttention (vLLM) virtual memory
- Continuous Batching / In-flight batching
- Speculative Decoding (draft-verify)
- Flash Decoding for long contexts
- GGUF/GPTQ/AWQ quantization formats

### Flash Attention 3 (Hopper) â˜… NEW
- WGMMA and TMA hardware features
- Pingpong warpgroup scheduling
- Intra-warpgroup GEMM-softmax overlap
- FP8 with incoherent processing
- 740 TFLOPS (75% H100 peak)

## ğŸ“– Reference Resources

### Books
- "Computer Systems: A Programmer's Perspective" (CS:APP)
- "What Every Programmer Should Know About Memory" (Drepper)
- "The C Programming Language" (K&R)

### Online
- Godbolt Compiler Explorer: https://godbolt.org
- Intel Intrinsics Guide
- NVIDIA CUDA Documentation
- PyTorch Internals

## âœ… Completion Checklist

- [ ] Module 00: Crucial Concepts
- [ ] Module 01: Computer Architecture
- [ ] Module 02: Operating Systems
- [ ] Module 03: Assembly Programming
- [ ] Module 04: C Programming
- [ ] Module 05: C++ Programming
- [ ] Module 06: Hardware Fundamentals
- [ ] Module 07: Multimodal Data Formats
- [ ] Module 08: Device I/O
