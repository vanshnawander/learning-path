{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Internals - Hands-On Exploration\n",
    "\n",
    "This notebook provides interactive exploration of PyTorch's internal architecture.\n",
    "\n",
    "## Topics\n",
    "1. Tensor Data Structure\n",
    "2. Storage and Views\n",
    "3. Autograd Graph\n",
    "4. Dispatcher Behavior\n",
    "5. Profiling Internals\n",
    "6. Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensor Data Structure (TensorImpl)\n",
    "\n",
    "Every tensor has metadata stored in TensorImpl:\n",
    "- sizes (shape)\n",
    "- strides (memory layout)\n",
    "- storage_offset\n",
    "- dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor and inspect its internals\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Tensor:\")\n",
    "print(x)\n",
    "print(f\"\\nShape: {x.shape}\")\n",
    "print(f\"Strides: {x.stride()}\")\n",
    "print(f\"Storage offset: {x.storage_offset()}\")\n",
    "print(f\"Total elements: {x.numel()}\")\n",
    "print(f\"Is contiguous: {x.is_contiguous()}\")\n",
    "print(f\"\\nDtype: {x.dtype}\")\n",
    "print(f\"Device: {x.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding strides\n",
    "# Strides tell you how many elements to skip in memory to move one step in each dimension\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original tensor: shape={x.shape}, strides={x.stride()}\")\n",
    "print(f\"To move one row: skip {x.stride()[0]} elements\")\n",
    "print(f\"To move one column: skip {x.stride()[1]} elements\")\n",
    "\n",
    "# Index calculation: tensor[i, j] = storage[offset + i*stride[0] + j*stride[1]]\n",
    "i, j = 1, 2\n",
    "offset = x.storage_offset() + i * x.stride()[0] + j * x.stride()[1]\n",
    "print(f\"\\ntensor[{i}, {j}] = {x[i, j].item()}\")\n",
    "print(f\"Calculated offset: {offset}\")\n",
    "print(f\"storage[{offset}] = {x.storage()[offset]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Storage and Views\n",
    "\n",
    "Views share the same underlying storage. Modifying one affects the other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Views share storage\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = x[1, :]  # View of second row\n",
    "\n",
    "print(f\"x:\\n{x}\")\n",
    "print(f\"\\ny = x[1, :] = {y}\")\n",
    "print(f\"\\ny.storage_offset() = {y.storage_offset()}\")\n",
    "print(f\"Same storage? {x.storage().data_ptr() == y.storage().data_ptr()}\")\n",
    "\n",
    "# Modify y\n",
    "y[0] = 100\n",
    "print(f\"\\nAfter y[0] = 100:\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"x =\\n{x}\")\n",
    "print(\"x was also modified because y is a view!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose changes strides, not data\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x_t = x.t()\n",
    "\n",
    "print(f\"x shape: {x.shape}, strides: {x.stride()}\")\n",
    "print(f\"x.t() shape: {x_t.shape}, strides: {x_t.stride()}\")\n",
    "print(f\"\\nx.t() is NOT contiguous: {x_t.is_contiguous()}\")\n",
    "print(f\"Same storage: {x.storage().data_ptr() == x_t.storage().data_ptr()}\")\n",
    "\n",
    "# Make contiguous creates a copy\n",
    "x_t_contig = x_t.contiguous()\n",
    "print(f\"\\nx.t().contiguous() is contiguous: {x_t_contig.is_contiguous()}\")\n",
    "print(f\"Different storage: {x.storage().data_ptr() != x_t_contig.storage().data_ptr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autograd Graph\n",
    "\n",
    "PyTorch builds a computation graph dynamically during forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the autograd graph\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Build computation graph\n",
    "z = x * y  # MulBackward\n",
    "w = z + 1  # AddBackward\n",
    "loss = w ** 2  # PowBackward\n",
    "\n",
    "print(\"Computation: loss = ((x * y) + 1)^2\")\n",
    "print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "print(f\"loss = {loss.item()}\")\n",
    "\n",
    "print(f\"\\n--- Autograd Graph ---\")\n",
    "print(f\"loss.grad_fn = {loss.grad_fn}\")\n",
    "print(f\"  └── {loss.grad_fn.next_functions[0][0]}\")\n",
    "print(f\"      └── {loss.grad_fn.next_functions[0][0].next_functions[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"After backward():\")\n",
    "print(f\"x.grad = {x.grad.item()}\")\n",
    "print(f\"y.grad = {y.grad.item()}\")\n",
    "\n",
    "# Manual calculation:\n",
    "# loss = ((x*y) + 1)^2\n",
    "# dloss/dx = 2*((x*y)+1) * y = 2*(6+1)*3 = 42\n",
    "# dloss/dy = 2*((x*y)+1) * x = 2*(6+1)*2 = 28\n",
    "print(f\"\\nManual verification:\")\n",
    "print(f\"dloss/dx = 2*((x*y)+1)*y = 2*7*3 = 42 ✓\")\n",
    "print(f\"dloss/dy = 2*((x*y)+1)*x = 2*7*2 = 28 ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom autograd function\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "# Test it\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "y = MyReLU.apply(x)\n",
    "y.sum().backward()\n",
    "\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"MyReLU(x) = {y.data}\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(\"Gradient is 0 where x < 0, 1 where x >= 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dispatcher Behavior\n",
    "\n",
    "Use `__torch_dispatch__` to intercept all operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "\n",
    "class OperationLogger(TorchDispatchMode):\n",
    "    def __init__(self):\n",
    "        self.operations = []\n",
    "    \n",
    "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
    "        kwargs = kwargs or {}\n",
    "        self.operations.append({\n",
    "            'name': str(func),\n",
    "            'input_shapes': [a.shape if hasattr(a, 'shape') else type(a).__name__ for a in args]\n",
    "        })\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "# Log operations in a forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "x = torch.randn(4, 10)\n",
    "\n",
    "with OperationLogger() as logger:\n",
    "    y = model(x)\n",
    "\n",
    "print(f\"Operations in forward pass:\")\n",
    "for i, op in enumerate(logger.operations[:10]):  # First 10\n",
    "    print(f\"  {i+1}. {op['name']}\")\n",
    "print(f\"  ... ({len(logger.operations)} total operations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Profiling Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile operations\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(512, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512)\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    x = torch.randn(32, 512, device='cuda')\n",
    "else:\n",
    "    x = torch.randn(32, 512)\n",
    "\n",
    "# Profile with PyTorch profiler\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ] if torch.cuda.is_available() else [torch.profiler.ProfilerActivity.CPU],\n",
    "    record_shapes=True,\n",
    ") as prof:\n",
    "    for _ in range(10):\n",
    "        y = model(x)\n",
    "        if x.requires_grad:\n",
    "            y.sum().backward()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Reset memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Initial state:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Allocate tensors\n",
    "    tensors = [torch.randn(1000, 1000, device='cuda') for _ in range(5)]\n",
    "    \n",
    "    print(\"\\nAfter allocating 5 tensors:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Delete tensors\n",
    "    del tensors\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"\\nAfter deleting (memory cached, not freed):\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Empty cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nAfter empty_cache():\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved() / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient checkpointing for memory efficiency\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class HeavyBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "\n",
    "dim = 512\n",
    "num_layers = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "layers = nn.ModuleList([HeavyBlock(dim) for _ in range(num_layers)]).to(device)\n",
    "\n",
    "def forward_normal(x, layers):\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "def forward_checkpoint(x, layers):\n",
    "    for layer in layers:\n",
    "        x = checkpoint(layer, x, use_reentrant=False)\n",
    "    return x\n",
    "\n",
    "x = torch.randn(32, dim, device=device, requires_grad=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Normal forward\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    y = forward_normal(x.clone().requires_grad_(True), layers)\n",
    "    y.sum().backward()\n",
    "    mem_normal = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    # Checkpoint forward\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    y = forward_checkpoint(x.clone().requires_grad_(True), layers)\n",
    "    y.sum().backward()\n",
    "    mem_checkpoint = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(f\"Peak memory (normal): {mem_normal:.1f} MB\")\n",
    "    print(f\"Peak memory (checkpoint): {mem_checkpoint:.1f} MB\")\n",
    "    print(f\"Savings: {(1 - mem_checkpoint/mem_normal)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"CUDA not available for memory comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key PyTorch internals explored:\n",
    "\n",
    "1. **TensorImpl** - Core data structure with sizes, strides, storage\n",
    "2. **Storage** - Actual memory, shared by views\n",
    "3. **Autograd** - Dynamic computation graph with grad_fn nodes\n",
    "4. **Dispatcher** - Routes operations to implementations\n",
    "5. **Profiler** - Understand performance characteristics\n",
    "6. **Memory** - Caching allocator, checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                  PYTORCH INTERNALS SUMMARY                       ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  TENSOR = TensorImpl + Storage                                   ║\n",
    "║    • sizes, strides define logical view                          ║\n",
    "║    • storage holds actual data                                   ║\n",
    "║    • views share storage                                         ║\n",
    "║                                                                  ║\n",
    "║  AUTOGRAD = Dynamic computation graph                            ║\n",
    "║    • grad_fn links to backward function                          ║\n",
    "║    • gradients accumulate in leaf tensors                        ║\n",
    "║    • custom functions via torch.autograd.Function                ║\n",
    "║                                                                  ║\n",
    "║  DISPATCHER = Multi-level routing                                ║\n",
    "║    • DispatchKey identifies functionality                        ║\n",
    "║    • Priority determines execution order                         ║\n",
    "║    • __torch_dispatch__ for Python hooks                         ║\n",
    "║                                                                  ║\n",
    "║  MEMORY = Caching allocator                                      ║\n",
    "║    • Avoids cudaMalloc overhead                                  ║\n",
    "║    • empty_cache() returns to system                             ║\n",
    "║    • Checkpointing trades compute for memory                     ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
