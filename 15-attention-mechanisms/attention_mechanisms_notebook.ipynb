{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms - Hands-On Exploration\n",
    "\n",
    "This notebook provides interactive exploration of attention mechanisms.\n",
    "\n",
    "## Topics\n",
    "1. Basic Attention Implementation\n",
    "2. Multi-Head Attention\n",
    "3. Attention Visualization\n",
    "4. Flash Attention Comparison\n",
    "5. Efficient Attention Variants\n",
    "6. Profiling and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Scaled Dot-Product Attention\n",
    "\n",
    "The core formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Basic scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: (batch, seq_len, d_k)\n",
    "        K: (batch, seq_len, d_k)\n",
    "        V: (batch, seq_len, d_v)\n",
    "        mask: (batch, seq_len, seq_len) or (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply mask (optional)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 3: Softmax to get attention weights\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 4: Apply attention to values\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# Test it\n",
    "batch_size, seq_len, d_model = 2, 6, 64\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shapes: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights sum to 1: {weights[0, 0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(weights[0].detach().numpy(), cmap='Blues')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights (Batch 0)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row sums to 1 (probability distribution over keys)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Causal (Masked) Attention\n",
    "\n",
    "For autoregressive models, each position can only attend to previous positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal mask\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create lower-triangular mask for causal attention.\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)\n",
    "\n",
    "# Apply causal attention\n",
    "output_causal, weights_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].imshow(weights[0].detach().numpy(), cmap='Blues')\n",
    "axes[0].set_title('Full Attention')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "axes[1].imshow(weights_causal[0].detach().numpy(), cmap='Blues')\n",
    "axes[1].set_title('Causal Attention (Masked)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Causal attention has zeros above diagonal (no future info)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention\n",
    "\n",
    "Multiple attention heads allow the model to attend to different aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project and reshape to (batch, heads, seq, head_dim)\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concat heads and project\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test\n",
    "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, attn = mha(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Attention per head: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(attn[0, i].detach().numpy(), cmap='Blues')\n",
    "    axes[i].set_title(f'Head {i}')\n",
    "    axes[i].set_xlabel('Key')\n",
    "    axes[i].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Different Attention Heads Learn Different Patterns', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Flash Attention vs Standard Attention\n",
    "\n",
    "Compare memory and speed of Flash Attention (via SDPA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_attention(seq_len, batch=4, heads=8, head_dim=64, iterations=20):\n",
    "    \"\"\"Profile standard vs flash attention.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None\n",
    "    \n",
    "    q = torch.randn(batch, heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    k = torch.randn(batch, heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    v = torch.randn(batch, heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Standard attention\n",
    "    def standard():\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, v)\n",
    "    \n",
    "    # Flash attention\n",
    "    def flash():\n",
    "        return F.scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = standard()\n",
    "        _ = flash()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Time standard\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    for _ in range(iterations):\n",
    "        _ = standard()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    std_time = start.elapsed_time(end) / iterations\n",
    "    \n",
    "    # Time flash\n",
    "    start.record()\n",
    "    for _ in range(iterations):\n",
    "        _ = flash()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    flash_time = start.elapsed_time(end) / iterations\n",
    "    \n",
    "    return std_time, flash_time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Sequence Length | Standard (ms) | Flash (ms) | Speedup\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for seq_len in [512, 1024, 2048, 4096]:\n",
    "        std_time, flash_time = profile_attention(seq_len)\n",
    "        if std_time:\n",
    "            speedup = std_time / flash_time\n",
    "            print(f\"{seq_len:^15} | {std_time:^13.2f} | {flash_time:^10.2f} | {speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"CUDA not available for profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Attention\n",
    "\n",
    "O(N) complexity by removing softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_attention(Q, K, V, feature_map='elu'):\n",
    "    \"\"\"\n",
    "    Linear attention: O(N) complexity.\n",
    "    \n",
    "    Key insight: φ(Q) @ (φ(K)^T @ V) instead of softmax(QK^T) @ V\n",
    "    \"\"\"\n",
    "    # Apply feature map (must be non-negative)\n",
    "    if feature_map == 'elu':\n",
    "        Q = F.elu(Q) + 1\n",
    "        K = F.elu(K) + 1\n",
    "    elif feature_map == 'relu':\n",
    "        Q = F.relu(Q)\n",
    "        K = F.relu(K)\n",
    "    \n",
    "    # Linear attention: Q @ (K^T @ V)\n",
    "    # Compute K^T @ V first: O(d²) instead of O(N²)\n",
    "    KV = torch.einsum('bnd,bnv->bdv', K, V)  # (batch, d_k, d_v)\n",
    "    \n",
    "    # Q @ KV\n",
    "    output = torch.einsum('bnd,bdv->bnv', Q, KV)  # (batch, seq, d_v)\n",
    "    \n",
    "    # Normalize\n",
    "    K_sum = K.sum(dim=1, keepdim=True)  # (batch, 1, d)\n",
    "    normalizer = torch.einsum('bnd,bkd->bnk', Q, K_sum).squeeze(-1)  # (batch, seq)\n",
    "    output = output / (normalizer.unsqueeze(-1) + 1e-6)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Compare standard vs linear\n",
    "Q = torch.randn(2, 100, 64)\n",
    "K = torch.randn(2, 100, 64)\n",
    "V = torch.randn(2, 100, 64)\n",
    "\n",
    "out_standard, _ = scaled_dot_product_attention(Q, K, V)\n",
    "out_linear = linear_attention(Q, K, V)\n",
    "\n",
    "print(f\"Standard output shape: {out_standard.shape}\")\n",
    "print(f\"Linear output shape: {out_linear.shape}\")\n",
    "print(f\"\\nNote: Linear attention is an APPROXIMATION, not exact!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attention Statistics and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_stats(attn_weights):\n",
    "    \"\"\"\n",
    "    Compute useful statistics for debugging attention.\n",
    "    \n",
    "    Args:\n",
    "        attn_weights: (batch, seq, seq) or (batch, heads, seq, seq)\n",
    "    \"\"\"\n",
    "    # Flatten heads if present\n",
    "    if attn_weights.dim() == 4:\n",
    "        attn_weights = attn_weights.mean(dim=1)  # Average over heads\n",
    "    \n",
    "    # Entropy (higher = more uniform)\n",
    "    entropy = -(attn_weights * torch.log(attn_weights + 1e-10)).sum(dim=-1)\n",
    "    \n",
    "    # Max attention weight per query\n",
    "    max_attn = attn_weights.max(dim=-1).values\n",
    "    \n",
    "    # Effective context (exp of entropy)\n",
    "    eff_context = torch.exp(entropy)\n",
    "    \n",
    "    # Diagonal dominance (self-attention)\n",
    "    seq_len = attn_weights.size(-1)\n",
    "    diagonal = torch.diagonal(attn_weights, dim1=-2, dim2=-1)\n",
    "    \n",
    "    return {\n",
    "        'entropy': entropy.mean().item(),\n",
    "        'max_attention': max_attn.mean().item(),\n",
    "        'effective_context': eff_context.mean().item(),\n",
    "        'diagonal_attention': diagonal.mean().item()\n",
    "    }\n",
    "\n",
    "# Generate different attention patterns\n",
    "seq_len = 20\n",
    "\n",
    "# Random attention\n",
    "Q_rand = torch.randn(1, seq_len, 64)\n",
    "K_rand = torch.randn(1, seq_len, 64)\n",
    "V_rand = torch.randn(1, seq_len, 64)\n",
    "_, attn_rand = scaled_dot_product_attention(Q_rand, K_rand, V_rand)\n",
    "\n",
    "# Identity-like (Q = K)\n",
    "Q_id = torch.randn(1, seq_len, 64)\n",
    "_, attn_id = scaled_dot_product_attention(Q_id, Q_id * 10, V_rand)  # Scale for peaky\n",
    "\n",
    "print(\"Attention Pattern Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nRandom Q, K:\")\n",
    "for k, v in compute_attention_stats(attn_rand).items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "print(f\"\\nIdentity-like (Q ≈ K):\")\n",
    "for k, v in compute_attention_stats(attn_id).items():\n",
    "    print(f\"  {k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(attn_rand[0].detach().numpy(), cmap='Blues')\n",
    "axes[0].set_title('Random Attention (distributed)')\n",
    "axes[0].set_xlabel('Key')\n",
    "axes[0].set_ylabel('Query')\n",
    "\n",
    "axes[1].imshow(attn_id[0].detach().numpy(), cmap='Blues')\n",
    "axes[1].set_title('Identity-like Attention (diagonal)')\n",
    "axes[1].set_xlabel('Key')\n",
    "axes[1].set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Position Encodings Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_position_encoding(seq_len, d_model):\n",
    "    \"\"\"Sinusoidal position encoding from 'Attention Is All You Need'.\"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "    \n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Generate position encodings\n",
    "pe = sinusoidal_position_encoding(100, 64)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Position encoding heatmap\n",
    "axes[0].imshow(pe.numpy(), aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Sinusoidal Position Encoding')\n",
    "\n",
    "# Similarity between positions\n",
    "similarity = torch.mm(pe, pe.t())\n",
    "axes[1].imshow(similarity.numpy(), cmap='RdBu')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Position')\n",
    "axes[1].set_title('Position Similarity (dot product)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Nearby positions have higher similarity (visible in right plot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key concepts explored:\n",
    "\n",
    "1. **Scaled Dot-Product Attention** - Core formula with scaling\n",
    "2. **Causal Masking** - For autoregressive models\n",
    "3. **Multi-Head Attention** - Multiple attention perspectives\n",
    "4. **Flash Attention** - IO-aware efficient implementation\n",
    "5. **Linear Attention** - O(N) approximation\n",
    "6. **Debugging** - Statistics and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                ATTENTION MECHANISMS SUMMARY                      ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  STANDARD ATTENTION: O(N²) time and memory                       ║\n",
    "║    Attention(Q,K,V) = softmax(QK^T / √d) · V                     ║\n",
    "║                                                                  ║\n",
    "║  MULTI-HEAD: h parallel attention heads                          ║\n",
    "║    More expressive, same asymptotic cost                         ║\n",
    "║                                                                  ║\n",
    "║  FLASH ATTENTION: Same output, O(N) memory                       ║\n",
    "║    Use F.scaled_dot_product_attention()                          ║\n",
    "║                                                                  ║\n",
    "║  LINEAR ATTENTION: O(N) time                                     ║\n",
    "║    Approximation - trades quality for speed                      ║\n",
    "║                                                                  ║\n",
    "║  DEBUGGING: Check entropy, visualize patterns                    ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
