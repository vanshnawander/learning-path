# üöÄ Complete Systems & HPC Learning Path

A comprehensive learning journey from **Computer Architecture fundamentals** to **High-Performance Computing mastery**.

## üìã Learning Path Overview

This learning path is designed to take you from foundational concepts to advanced HPC and ML systems programming. Each module builds upon the previous ones.

## üìÅ Module Structure

| # | Module | Description |
|---|--------|-------------|
| 00 | Prerequisites | Mathematics, Programming Fundamentals |
| 01 | Computer Architecture | CPU design, Memory hierarchy, ISA |
| 02 | Operating Systems | Processes, Memory management, I/O |
| 03 | Assembly Programming | x86-64, ARM, Low-level programming |
| 04 | C Programming | Systems programming in C |
| 05 | C++ Programming | Modern C++, Templates, Memory |
| 06 | Memory Management | Allocation, Caching, Virtual memory |
| 07 | Data Structures & Algorithms | Performance-critical implementations |
| 08 | Parallel Computing Fundamentals | Threads, Synchronization, OpenMP |
| 09 | Data Formats & Serialization | Binary formats, Beton, WebDataset, Arrow |
| 10 | Data Loading Pipelines | FFCV, DALI, Efficient data loading |
| 11 | GPU Architecture | NVIDIA GPU internals, Memory hierarchy |
| 12 | CUDA Programming | Kernels, Memory, Optimization |
| 13 | Triton Programming | High-level GPU programming |
| 14 | ML Compilers | TVM, XLA, MLIR, Graph optimization |
| 15 | PyTorch Internals | Autograd, Dispatch, ATen |
| 16 | Attention Mechanisms | Flash Attention, Memory-efficient attention |
| 17 | Training Optimization | Mixed precision, Gradient checkpointing |
| 18 | Distributed Computing | Multi-GPU, Multi-node training |
| 19 | HPC & Performance Engineering | Profiling, Optimization, Benchmarking |
| 20 | Advanced Topics | Custom hardware, Mojo, Future directions |

## üéØ Learning Approach

1. **Theory First**: Understand the concepts
2. **Hands-On Practice**: Implement in code
3. **Read Source Code**: Study production implementations
4. **Benchmark & Profile**: Measure everything
5. **Optimize**: Apply learnings to real problems

## üìö Available Repositories

The following repositories are available for reference:
- `pytorch/` - PyTorch source code
- `flash-attention/` - Flash Attention implementation
- `unsloth/` - Efficient LLM fine-tuning
- `ffcv-main/` - Fast Forward Computer Vision data loading
- `mirage/` - ML compiler research

## üõ†Ô∏è Tools You'll Master

- **Languages**: C, C++, CUDA, Python, Triton, Assembly, Mojo
- **Frameworks**: PyTorch, TVM, MLIR
- **Profilers**: Nsight, perf, VTune
- **Build Systems**: CMake, Ninja, setuptools

---
*Start with `00-prerequisites/` and work your way through systematically.*
