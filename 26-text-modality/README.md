# Text Modality: Complete Learning Path

A comprehensive curriculum for understanding text processing, tokenization, language models, and LLM systems. Built from foundational NLP to cutting-edge research (GPT-4, Llama 3, DeepSeek V3).

## Curriculum Philosophy

- **Core Depth Over Abstractions**: Every concept includes profiled implementations
- **Research-Grounded**: Each module references seminal papers and latest research
- **Practical Focus**: Runnable code, benchmarks, and production considerations

---

## Module Overview (50+ Files Planned)

```
26-text-modality/
â”œâ”€â”€ 01-foundations/                      # Core text concepts (8 files)
â”‚   â”œâ”€â”€ 00_nlp_history_statistical_to_neural.md  # N-grams â†’ RNN â†’ Transformers
â”‚   â”œâ”€â”€ 01_text_preprocessing_fundamentals.md    # Cleaning, normalization, regex
â”‚   â”œâ”€â”€ 02_text_preprocessing_profiled.py        # Benchmarked implementations
â”‚   â”œâ”€â”€ 03_unicode_encodings_deep_dive.md        # UTF-8, UTF-16, normalization forms
â”‚   â”œâ”€â”€ 04_regular_expressions_nlp.py            # Regex patterns for NLP
â”‚   â”œâ”€â”€ 05_text_fundamentals.c                   # Pure C string processing
â”‚   â”œâ”€â”€ 06_linguistic_features.md                # POS, NER, dependency parsing
â”‚   â””â”€â”€ 07_spacy_nltk_comparison.py              # Library comparison with benchmarks
â”‚
â”œâ”€â”€ 02-tokenization/                     # Tokenization deep dive (10 files)
â”‚   â”œâ”€â”€ 01_tokenization_fundamentals.md          # Why tokenization matters
â”‚   â”œâ”€â”€ 02_bpe_algorithm_deep_dive.md            # Byte-Pair Encoding from scratch
â”‚   â”œâ”€â”€ 03_bpe_implementation.py                 # Complete BPE implementation
â”‚   â”œâ”€â”€ 04_wordpiece_algorithm.md                # WordPiece (BERT tokenizer)
â”‚   â”œâ”€â”€ 05_unigram_sentencepiece.md              # Unigram LM tokenization
â”‚   â”œâ”€â”€ 06_sentencepiece_implementation.py       # SentencePiece training/inference
â”‚   â”œâ”€â”€ 07_tiktoken_analysis.md                  # OpenAI's tiktoken internals
â”‚   â”œâ”€â”€ 08_tiktoken_custom_training.py           # Custom vocabulary training
â”‚   â”œâ”€â”€ 09_tokenizer_comparison_benchmark.py     # Speed/quality comparison
â”‚   â””â”€â”€ 10_byte_level_bpe.md                     # GPT-2/3/4 style tokenization
â”‚
â”œâ”€â”€ 03-embeddings/                       # Text representations (8 files)
â”‚   â”œâ”€â”€ 01_word_embeddings_history.md            # One-hot â†’ Word2Vec â†’ Transformers
â”‚   â”œâ”€â”€ 02_word2vec_implementation.py            # CBOW and Skip-gram from scratch
â”‚   â”œâ”€â”€ 03_glove_fasttext.md                     # GloVe, FastText architectures
â”‚   â”œâ”€â”€ 04_contextual_embeddings.md              # ELMo â†’ BERT contextualization
â”‚   â”œâ”€â”€ 05_sentence_transformers.md              # Sentence-BERT, contrastive learning
â”‚   â”œâ”€â”€ 06_sentence_embeddings_profiled.py       # Embedding generation benchmarks
â”‚   â”œâ”€â”€ 07_embedding_similarity_search.py        # Cosine similarity, FAISS, ANN
â”‚   â””â”€â”€ 08_embedding_visualization.ipynb         # t-SNE, UMAP visualization
â”‚
â”œâ”€â”€ 04-attention-mechanisms/             # Attention deep dive (7 files)
â”‚   â”œâ”€â”€ 01_attention_fundamentals.md             # Seq2seq attention origins
â”‚   â”œâ”€â”€ 02_self_attention_math.md                # QKV, scaled dot-product
â”‚   â”œâ”€â”€ 03_multi_head_attention.py               # MHA implementation from scratch
â”‚   â”œâ”€â”€ 04_attention_variants.md                 # MQA, GQA, sliding window
â”‚   â”œâ”€â”€ 05_flash_attention_explained.md          # Memory-efficient attention
â”‚   â”œâ”€â”€ 06_flash_attention_cuda.cu               # CUDA FlashAttention kernel
â”‚   â””â”€â”€ 07_attention_visualization.ipynb         # Attention pattern analysis
â”‚
â”œâ”€â”€ 05-transformer-architecture/         # Transformer internals (8 files)
â”‚   â”œâ”€â”€ 01_transformer_original_paper.md         # "Attention Is All You Need"
â”‚   â”œâ”€â”€ 02_encoder_decoder_architecture.md       # Full architecture breakdown
â”‚   â”œâ”€â”€ 03_transformer_from_scratch.py           # Complete implementation
â”‚   â”œâ”€â”€ 04_positional_encodings.md               # Sinusoidal, learned, RoPE, ALiBi
â”‚   â”œâ”€â”€ 05_rope_implementation.py                # Rotary Position Embedding
â”‚   â”œâ”€â”€ 06_layer_normalization.md                # Pre-LN vs Post-LN, RMSNorm
â”‚   â”œâ”€â”€ 07_feed_forward_networks.md              # FFN, GLU, SwiGLU variants
â”‚   â””â”€â”€ 08_transformer_profiled.py               # Performance analysis
â”‚
â”œâ”€â”€ 06-language-models/                  # LM architectures (10 files)
â”‚   â”œâ”€â”€ 01_language_modeling_fundamentals.md     # Perplexity, autoregressive LM
â”‚   â”œâ”€â”€ 02_bert_architecture.md                  # BERT, masked LM, NSP
â”‚   â”œâ”€â”€ 03_gpt_architecture_evolution.md         # GPT-1 â†’ GPT-2 â†’ GPT-3 â†’ GPT-4
â”‚   â”œâ”€â”€ 04_llama_architecture.md                 # Llama 1/2/3, architectural choices
â”‚   â”œâ”€â”€ 05_mistral_mixtral.md                    # Mistral, Mixtral MoE
â”‚   â”œâ”€â”€ 06_deepseek_architecture.md              # DeepSeek V2/V3, MLA attention
â”‚   â”œâ”€â”€ 07_moe_mixture_of_experts.md             # Sparse MoE, routing, load balancing
â”‚   â”œâ”€â”€ 08_llm_comparison_table.md               # Architecture comparison matrix
â”‚   â”œâ”€â”€ 09_small_language_models.md              # Phi, Gemma, efficient LLMs
â”‚   â””â”€â”€ 10_llm_from_scratch.py                   # Mini-LLM implementation
â”‚
â”œâ”€â”€ 07-training-methods/                 # Training techniques (8 files)
â”‚   â”œâ”€â”€ 01_pretraining_objectives.md             # CLM, MLM, span corruption
â”‚   â”œâ”€â”€ 02_instruction_tuning.md                 # SFT, instruction datasets
â”‚   â”œâ”€â”€ 03_rlhf_explained.md                     # RLHF pipeline, reward modeling
â”‚   â”œâ”€â”€ 04_dpo_direct_preference.md              # DPO, IPO, KTO alternatives
â”‚   â”œâ”€â”€ 05_lora_qlora.md                         # Parameter-efficient fine-tuning
â”‚   â”œâ”€â”€ 06_lora_implementation.py                # LoRA from scratch
â”‚   â”œâ”€â”€ 07_full_finetuning_vs_peft.md            # When to use what
â”‚   â””â”€â”€ 08_training_recipes.py                   # Complete training scripts
â”‚
â”œâ”€â”€ 08-inference-optimization/           # LLM inference (8 files)
â”‚   â”œâ”€â”€ 01_kv_cache_explained.md                 # KV caching mechanics
â”‚   â”œâ”€â”€ 02_kv_cache_implementation.py            # KV cache from scratch
â”‚   â”œâ”€â”€ 03_quantization_methods.md               # INT8, INT4, GPTQ, AWQ, GGUF
â”‚   â”œâ”€â”€ 04_quantization_benchmark.py             # Quality vs speed tradeoffs
â”‚   â”œâ”€â”€ 05_speculative_decoding.md               # Draft model acceleration
â”‚   â”œâ”€â”€ 06_continuous_batching.md                # vLLM, TensorRT-LLM batching
â”‚   â”œâ”€â”€ 07_vllm_tensorrt_comparison.py           # Inference engine benchmarks
â”‚   â””â”€â”€ 08_serving_optimization.md               # Production deployment
â”‚
â”œâ”€â”€ 09-text-generation/                  # Generation methods (6 files)
â”‚   â”œâ”€â”€ 01_decoding_strategies.md                # Greedy, beam, sampling
â”‚   â”œâ”€â”€ 02_sampling_methods.py                   # Top-k, top-p, temperature
â”‚   â”œâ”€â”€ 03_constrained_generation.md             # Structured output, JSON mode
â”‚   â”œâ”€â”€ 04_prompt_engineering.md                 # Few-shot, CoT, system prompts
â”‚   â”œâ”€â”€ 05_rag_retrieval_augmented.md            # RAG architecture
â”‚   â””â”€â”€ 06_rag_implementation.py                 # Complete RAG pipeline
â”‚
â”œâ”€â”€ 10-nlp-tasks/                        # Classic NLP tasks (6 files)
â”‚   â”œâ”€â”€ 01_text_classification.py                # Sentiment, topic classification
â”‚   â”œâ”€â”€ 02_named_entity_recognition.py           # NER with transformers
â”‚   â”œâ”€â”€ 03_question_answering.md                 # Extractive, generative QA
â”‚   â”œâ”€â”€ 04_summarization.md                      # Abstractive, extractive
â”‚   â”œâ”€â”€ 05_machine_translation.md                # Seq2seq, multilingual
â”‚   â””â”€â”€ 06_semantic_similarity.py                # STS benchmarks
â”‚
â”œâ”€â”€ 11-optimization-profiling/           # Performance engineering (4 files)
â”‚   â”œâ”€â”€ 01_text_data_loading.md                  # Efficient text datasets
â”‚   â”œâ”€â”€ 02_huggingface_datasets_profiled.py      # HF datasets optimization
â”‚   â”œâ”€â”€ 03_tokenizer_parallelization.py          # Parallel tokenization
â”‚   â””â”€â”€ 04_memory_optimization.md                # Gradient checkpointing, offloading
â”‚
â”œâ”€â”€ 12-practical-notebooks/              # Hands-on experiments (5 files)
â”‚   â”œâ”€â”€ 01_tokenizer_from_scratch.ipynb          # Build BPE tokenizer
â”‚   â”œâ”€â”€ 02_transformer_from_scratch.ipynb        # Build transformer
â”‚   â”œâ”€â”€ 03_finetune_llm_qlora.ipynb              # QLoRA fine-tuning
â”‚   â”œâ”€â”€ 04_exercises_and_solutions.py            # Graded exercises
â”‚   â””â”€â”€ 05_llm_inference_optimization.ipynb      # Optimization techniques
â”‚
â”œâ”€â”€ 13-advanced-topics/                  # Cutting-edge research (5 files)
â”‚   â”œâ”€â”€ 01_long_context_methods.md               # RoPE scaling, landmark attention
â”‚   â”œâ”€â”€ 02_multimodal_text_integration.md        # Text in VLMs, audio LLMs
â”‚   â”œâ”€â”€ 03_reasoning_models.md                   # Chain-of-thought, o1, R1
â”‚   â”œâ”€â”€ 04_agents_tool_use.md                    # Function calling, agents
â”‚   â””â”€â”€ 05_latest_research_2025.md               # Most recent developments
â”‚
â”œâ”€â”€ papers/                              # Reference materials
â”‚   â””â”€â”€ paper_summaries.md                       # All papers summarized
â”‚
â”œâ”€â”€ resources/                           # Learning resources
â”‚   â”œâ”€â”€ glossary.md                              # 100+ NLP terms defined
â”‚   â””â”€â”€ external_links.md                        # Datasets, tools, community
â”‚
â””â”€â”€ README.md                            # This file
```

---

## Learning Progression

### Phase 1: Foundations (Week 1-2)
| Module | Topic | Key Papers |
|--------|-------|------------|
| 01 | Text Preprocessing | - |
| 01 | Unicode & Encodings | - |
| 02 | Tokenization Fundamentals | [BPE 2015](https://arxiv.org/abs/1508.07909) |
| 02 | BPE, WordPiece, Unigram | [SentencePiece 2018](https://arxiv.org/abs/1808.06226) |

### Phase 2: Embeddings & Attention (Week 3-4)
| Module | Topic | Key Papers |
|--------|-------|------------|
| 03 | Word2Vec, GloVe | [Word2Vec 2013](https://arxiv.org/abs/1301.3781) |
| 03 | Sentence Transformers | [Sentence-BERT 2019](https://arxiv.org/abs/1908.10084) |
| 04 | Attention Mechanisms | [Attention 2014](https://arxiv.org/abs/1409.0473) |
| 04 | Flash Attention | [FlashAttention 2022](https://arxiv.org/abs/2205.14135) |

### Phase 3: Transformers (Week 5-6)
| Module | Topic | Key Papers |
|--------|-------|------------|
| 05 | Transformer Architecture | [AIAYN 2017](https://arxiv.org/abs/1706.03762) |
| 05 | Positional Encodings | [RoPE 2021](https://arxiv.org/abs/2104.09864) |
| 06 | BERT Architecture | [BERT 2018](https://arxiv.org/abs/1810.04805) |
| 06 | GPT Architecture | [GPT-2 2019](https://openai.com/research/better-language-models) |

### Phase 4: Modern LLMs (Week 7-9)
| Module | Topic | Key Papers |
|--------|-------|------------|
| 06 | Llama Architecture | [Llama 2 2023](https://arxiv.org/abs/2307.09288) |
| 06 | Mixture of Experts | [Mixtral 2024](https://arxiv.org/abs/2401.04088) |
| 06 | DeepSeek | [DeepSeek V3 2024](https://arxiv.org/abs/2412.19437) |
| 07 | RLHF & DPO | [DPO 2023](https://arxiv.org/abs/2305.18290) |

### Phase 5: Training & Inference (Week 10-12)
| Module | Topic | Key Papers |
|--------|-------|------------|
| 07 | LoRA, QLoRA | [LoRA 2021](https://arxiv.org/abs/2106.09685) |
| 08 | KV Cache | - |
| 08 | Quantization | [GPTQ 2022](https://arxiv.org/abs/2210.17323) |
| 08 | Speculative Decoding | [Spec Decoding 2023](https://arxiv.org/abs/2302.01318) |

### Phase 6: Production (Week 13-14)
| Module | Topic | Resources |
|--------|-------|-----------|
| 09 | RAG Systems | - |
| 11 | Data Loading | - |
| 12 | Practical Notebooks | - |

---

## Key Research Papers

### Foundational (2013-2018)
1. **Word2Vec** (2013) - Efficient word embeddings
2. **GloVe** (2014) - Global vectors for word representation
3. **Attention** (2014) - Neural machine translation attention
4. **Transformer** (2017) - "Attention Is All You Need"
5. **BERT** (2018) - Bidirectional pre-training
6. **GPT** (2018) - Generative pre-training

### Modern LLMs (2019-2023)
7. **GPT-2/3** (2019/2020) - Scaling language models
8. **T5** (2019) - Text-to-text framework
9. **RoPE** (2021) - Rotary positional embeddings
10. **InstructGPT** (2022) - RLHF for alignment
11. **Llama** (2023) - Open-weight LLMs
12. **Mistral/Mixtral** (2023/2024) - Efficient architectures

### Cutting-Edge (2024-2025)
13. **Llama 3** (2024) - Latest Meta LLM
14. **DeepSeek V3** (2024) - MLA attention, efficient training
15. **Qwen 2.5** (2024) - Alibaba's LLM series
16. **o1/R1** (2024/2025) - Reasoning models

---

## Profiling Focus Areas

### Memory & Bandwidth
- Tokenization: Vocabulary size impact on memory
- Embeddings: Float32 vs Float16 vs INT8
- KV Cache: Memory growth with sequence length
- Batch processing vs streaming patterns

### Computation
- Attention: O(nÂ²) complexity, FlashAttention optimization
- FFN: Parameter count vs compute
- Inference: Prefill vs decode phases

### Model Comparisons
| Model | Parameters | Context | Architecture |
|-------|------------|---------|--------------|
| GPT-2 | 1.5B | 1K | Standard transformer |
| Llama 2 | 7-70B | 4K | RoPE, GQA |
| Mistral | 7B | 32K | Sliding window |
| Mixtral | 8x7B | 32K | Sparse MoE |
| DeepSeek V3 | 671B | 128K | MLA, MoE |

---

## Prerequisites

1. **Python**: Intermediate level
2. **PyTorch**: Basic tensor operations
3. **Linear Algebra**: Matrix operations, attention math
4. **Probability**: Language modeling basics

---

## Quick Start

```bash
# Setup environment
pip install torch transformers datasets tokenizers
pip install sentencepiece tiktoken
pip install accelerate bitsandbytes  # For efficient inference

# Clone reference implementations
git clone https://github.com/karpathy/nanoGPT
git clone https://github.com/huggingface/transformers
```

---

## Status Tracker

| Module | Status | Last Updated |
|--------|--------|--------------|
| 01-foundations | ðŸŸ¡ Planned | Dec 2024 |
| 02-tokenization | ðŸŸ¡ Planned | Dec 2024 |
| 03-embeddings | ðŸŸ¡ Planned | Dec 2024 |
| 04-attention-mechanisms | ðŸŸ¡ Planned | Dec 2024 |
| 05-transformer-architecture | ðŸŸ¡ Planned | Dec 2024 |
| 06-language-models | ðŸŸ¡ Planned | Dec 2024 |
| 07-training-methods | ðŸŸ¡ Planned | Dec 2024 |
| 08-inference-optimization | ðŸŸ¡ Planned | Dec 2024 |
| 09-text-generation | ðŸŸ¡ Planned | Dec 2024 |
| 10-nlp-tasks | ðŸŸ¡ Planned | Dec 2024 |
| 11-optimization-profiling | ðŸŸ¡ Planned | Dec 2024 |
| 12-practical-notebooks | ðŸŸ¡ Planned | Dec 2024 |
| 13-advanced-topics | ðŸŸ¡ Planned | Dec 2024 |

---

## Estimated Time: 14-16 weeks
