# 16 - Training Optimization

Comprehensive techniques for efficient model training, from mixed precision
to quantization to compilation. Based on real-world implementations from
Unsloth, HuggingFace, and PyTorch.

## ğŸ“š Modules Created

### Python Files

| File | Description |
|------|-------------|
| `mixed-precision/01_floating_point_formats.py` | FP32, FP16, BF16, FP8 deep dive |
| `mixed-precision/02_automatic_mixed_precision.py` | AMP, GradScaler, autocast |
| `memory/01_gradient_checkpointing.py` | Activation recomputation theory & practice |
| `memory/02_gradient_accumulation_8bit_optimizers.py` | Effective batch size, bitsandbytes |
| `fine-tuning/lora/01_lora_deep_dive.py` | LoRA, QLoRA, DoRA mathematics & implementation |
| `quantization/01_quantization_fundamentals.py` | PTQ, GPTQ, AWQ, inference formats |
| `fusion/01_operator_fusion.py` | RMSNorm, fused attention, fused MLP, Triton |
| `compilation/01_torch_compile.py` | TorchDynamo, Inductor, graph breaks |

## ğŸ”¬ Topics Covered In Depth

### Mixed Precision Training
- **FP32/FP16/BF16/FP8**: Bit representations, ranges, precision trade-offs
- **Loss Scaling**: Dynamic scaling, GradScaler internals
- **AMP**: autocast operation categories, common pitfalls
- **Hardware Support**: Tensor Cores, Ampere vs Hopper

### Memory Optimization
- **Gradient Checkpointing**: O(âˆšN) memory algorithm, selective strategies
- **Gradient Accumulation**: Effective batch size, LR scaling rules
- **8-bit Optimizers**: bitsandbytes Adam8bit, dynamic quantization
- **Paged Optimizers**: CPU offloading for peak memory

### Efficient Fine-Tuning (LoRA/QLoRA)
- **Low-Rank Decomposition**: Mathematical foundations, rank selection
- **LoRA Implementation**: Forward/backward pass, scaling factor Î±/r
- **QLoRA**: 4-bit NF4 quantization, double quantization
- **Advanced Variants**: DoRA, LoRA+, rsLoRA, AdaLoRA
- **Hyperparameters**: Rank, alpha, target modules, learning rates

### Quantization
- **Quantization Theory**: Affine vs symmetric, per-tensor vs per-channel
- **GPTQ**: Optimal Brain Quantization, Hessian-based updates
- **AWQ**: Activation-aware scaling, salient weight protection
- **Inference Formats**: GGUF, GGML, ExLlama, vLLM integration

### Operator Fusion
- **Memory Bandwidth**: Why fusion matters, arithmetic intensity
- **RMSNorm**: Simpler than LayerNorm, Triton implementation
- **Fused Attention**: QKV projection, Flash Attention integration
- **Fused MLP**: SwiGLU/GeGLU gate+up fusion
- **Fused Cross-Entropy**: Chunked computation for large vocabularies

### Compilation (torch.compile)
- **PyTorch 2.0 Stack**: TorchDynamo â†’ AOTAutograd â†’ TorchInductor
- **Compilation Modes**: default, reduce-overhead, max-autotune
- **Graph Breaks**: Causes, debugging, solutions
- **Inductor Optimizations**: Fusion, memory planning, Triton codegen

## ğŸ¯ Learning Objectives

- [x] Understand floating point formats and their trade-offs
- [x] Implement mixed precision training with AMP
- [x] Apply gradient checkpointing strategically
- [x] Use 8-bit optimizers for memory efficiency
- [x] Master LoRA/QLoRA fine-tuning
- [x] Understand quantization algorithms (GPTQ, AWQ)
- [x] Apply operator fusion principles
- [x] Use torch.compile effectively

## ğŸ’» Practical Exercises

1. Compare FP32/FP16/BF16 precision and speed
2. Implement gradient accumulation training loop
3. Fine-tune LLM with QLoRA on consumer GPU
4. Quantize model with GPTQ/AWQ
5. Profile fused vs unfused operations
6. Debug torch.compile graph breaks

## ğŸ“– Key Papers

- "Mixed Precision Training" (Micikevicius et al., 2017)
- "LoRA: Low-Rank Adaptation" (Hu et al., 2021)
- "QLoRA: Efficient Finetuning" (Dettmers et al., 2023)
- "GPTQ: Post-Training Quantization" (Frantar et al., 2022)
- "AWQ: Activation-aware Weight Quantization" (Lin et al., 2023)
- "8-bit Optimizers via Block-wise Quantization" (Dettmers et al., 2021)

## ğŸ”§ Code References

- `unsloth/unsloth/kernels/` - Production fused kernels (RMSNorm, LoRA, CrossEntropy)
- `unsloth/unsloth/models/` - Optimized model implementations
- `bitsandbytes/` - 8-bit optimizers and quantization
- `peft/` - Parameter-efficient fine-tuning library

## ğŸ“ Structure

```
16-training-optimization/
â”œâ”€â”€ README.md
â”œâ”€â”€ mixed-precision/
â”‚   â”œâ”€â”€ 01_floating_point_formats.py      # FP32, FP16, BF16, FP8
â”‚   â””â”€â”€ 02_automatic_mixed_precision.py   # AMP, GradScaler
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ 01_gradient_checkpointing.py      # Activation recomputation
â”‚   â””â”€â”€ 02_gradient_accumulation_8bit_optimizers.py
â”œâ”€â”€ fine-tuning/
â”‚   â””â”€â”€ lora/
â”‚       â””â”€â”€ 01_lora_deep_dive.py          # LoRA, QLoRA, DoRA
â”œâ”€â”€ quantization/
â”‚   â””â”€â”€ 01_quantization_fundamentals.py   # PTQ, GPTQ, AWQ
â”œâ”€â”€ fusion/
â”‚   â””â”€â”€ 01_operator_fusion.py             # RMSNorm, fused ops
â””â”€â”€ compilation/
    â””â”€â”€ 01_torch_compile.py               # TorchDynamo, Inductor
```

## ğŸ”„ Recommended Learning Path

```
1. Floating Point Formats     â†’ Understand the basics
2. Mixed Precision (AMP)      â†’ Apply to training
3. Gradient Checkpointing     â†’ Reduce activation memory
4. Gradient Accumulation      â†’ Scale batch size
5. 8-bit Optimizers          â†’ Reduce optimizer memory
6. LoRA/QLoRA                â†’ Efficient fine-tuning
7. Quantization              â†’ Inference optimization
8. Operator Fusion           â†’ Understanding Unsloth internals
9. torch.compile             â†’ Automatic optimization
```

## â±ï¸ Estimated Time

- Quick overview: 1-2 weeks
- Deep understanding: 4-5 weeks
- Hands-on mastery: 6-8 weeks
