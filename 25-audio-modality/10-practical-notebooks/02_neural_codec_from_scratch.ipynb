{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Audio Codec from Scratch\n",
    "\n",
    "Hands-on implementation of a neural audio codec with:\n",
    "- Encoder/Decoder architecture\n",
    "- Vector Quantization (VQ)\n",
    "- Residual Vector Quantization (RVQ)\n",
    "- Discriminator training\n",
    "- Full training loop with profiling\n",
    "\n",
    "This notebook implements a simplified version of SoundStream/EnCodec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchaudio matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Blocks\n",
    "\n",
    "### 1.1 Causal Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"1D convolution with causal padding (only past context).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 kernel_size: int, stride: int = 1, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=stride, dilation=dilation\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pad on the left (causal)\n",
    "        x = F.pad(x, (self.padding, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class CausalConvTranspose1d(nn.Module):\n",
    "    \"\"\"Causal transposed convolution for upsampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 kernel_size: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(\n",
    "            in_channels, out_channels, kernel_size, stride=stride\n",
    "        )\n",
    "        self.trim = kernel_size - stride\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        if self.trim > 0:\n",
    "            x = x[..., :-self.trim]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test causal conv\n",
    "conv = CausalConv1d(1, 32, kernel_size=7)\n",
    "x = torch.randn(1, 1, 100)\n",
    "y = conv(x)\n",
    "print(f\"Input: {x.shape} -> Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with dilated convolutions.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            CausalConv1d(channels, channels, kernel_size=3, dilation=dilation),\n",
    "            nn.ELU(),\n",
    "            CausalConv1d(channels, channels, kernel_size=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# Test\n",
    "block = ResidualBlock(32)\n",
    "x = torch.randn(1, 32, 100)\n",
    "y = block(x)\n",
    "print(f\"ResBlock: {x.shape} -> {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio encoder that compresses waveform to latent representation.\n",
    "    \n",
    "    Architecture:\n",
    "    - Initial conv\n",
    "    - Multiple downsample blocks (conv + residual)\n",
    "    - Final conv to latent dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 32,\n",
    "        latent_dim: int = 128,\n",
    "        strides: List[int] = [2, 4, 5, 8],  # Total: 2*4*5*8 = 320\n",
    "        num_residual: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.strides = strides\n",
    "        self.total_stride = np.prod(strides)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Initial conv\n",
    "        layers.append(CausalConv1d(1, channels, kernel_size=7))\n",
    "        \n",
    "        # Downsample blocks\n",
    "        in_ch = channels\n",
    "        for i, stride in enumerate(strides):\n",
    "            out_ch = min(in_ch * 2, 512)\n",
    "            \n",
    "            # Residual blocks with increasing dilation\n",
    "            for j in range(num_residual):\n",
    "                layers.append(ResidualBlock(in_ch, dilation=3**j))\n",
    "            \n",
    "            # Strided conv for downsampling\n",
    "            layers.append(nn.ELU())\n",
    "            layers.append(CausalConv1d(in_ch, out_ch, kernel_size=stride*2, stride=stride))\n",
    "            \n",
    "            in_ch = out_ch\n",
    "        \n",
    "        # Final residual blocks\n",
    "        for j in range(num_residual):\n",
    "            layers.append(ResidualBlock(in_ch, dilation=3**j))\n",
    "        \n",
    "        # Project to latent dim\n",
    "        layers.append(nn.ELU())\n",
    "        layers.append(CausalConv1d(in_ch, latent_dim, kernel_size=3))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x: (batch, 1, samples) -> (batch, latent_dim, frames)\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "# Test encoder\n",
    "encoder = Encoder(channels=32, latent_dim=128)\n",
    "x = torch.randn(1, 1, 16000)  # 1 second @ 16kHz\n",
    "z = encoder(x)\n",
    "print(f\"Encoder: {x.shape} -> {z.shape}\")\n",
    "print(f\"Compression: {x.shape[2]} -> {z.shape[2]} (stride={x.shape[2]//z.shape[2]}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantization with EMA codebook update.\n",
    "    \n",
    "    Maps continuous vectors to discrete codebook entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        codebook_size: int = 1024,\n",
    "        codebook_dim: int = 128,\n",
    "        commitment_weight: float = 0.25,\n",
    "        ema_decay: float = 0.99,\n",
    "        epsilon: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.codebook_size = codebook_size\n",
    "        self.codebook_dim = codebook_dim\n",
    "        self.commitment_weight = commitment_weight\n",
    "        self.ema_decay = ema_decay\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Codebook (EMA updated, not gradient trained)\n",
    "        self.register_buffer('embedding', torch.randn(codebook_size, codebook_dim))\n",
    "        self.register_buffer('cluster_size', torch.zeros(codebook_size))\n",
    "        self.register_buffer('embed_avg', self.embedding.clone())\n",
    "        \n",
    "        # Initialize with unit norm\n",
    "        self.embedding.data.uniform_(-1.0 / codebook_size, 1.0 / codebook_size)\n",
    "    \n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: (batch, dim, time) encoder output\n",
    "        Returns:\n",
    "            z_q: quantized output (same shape as z)\n",
    "            loss: commitment + codebook loss\n",
    "            indices: codebook indices (batch, time)\n",
    "        \"\"\"\n",
    "        B, D, T = z.shape\n",
    "        \n",
    "        # Reshape: (B, D, T) -> (B*T, D)\n",
    "        z_flat = z.permute(0, 2, 1).reshape(-1, D)\n",
    "        \n",
    "        # Compute distances to all codebook entries\n",
    "        # ||z - e||^2 = ||z||^2 + ||e||^2 - 2<z, e>\n",
    "        dist = (\n",
    "            z_flat.pow(2).sum(1, keepdim=True)\n",
    "            + self.embedding.pow(2).sum(1)\n",
    "            - 2 * z_flat @ self.embedding.t()\n",
    "        )\n",
    "        \n",
    "        # Find nearest codebook entry\n",
    "        indices = dist.argmin(dim=1)  # (B*T,)\n",
    "        \n",
    "        # Lookup quantized vectors\n",
    "        z_q_flat = self.embedding[indices]  # (B*T, D)\n",
    "        \n",
    "        # EMA codebook update (during training)\n",
    "        if self.training:\n",
    "            # One-hot encoding\n",
    "            encodings = F.one_hot(indices, self.codebook_size).float()  # (B*T, K)\n",
    "            \n",
    "            # Update cluster sizes\n",
    "            self.cluster_size.data.mul_(self.ema_decay).add_(\n",
    "                encodings.sum(0), alpha=1 - self.ema_decay\n",
    "            )\n",
    "            \n",
    "            # Update embedding averages\n",
    "            embed_sum = encodings.t() @ z_flat  # (K, D)\n",
    "            self.embed_avg.data.mul_(self.ema_decay).add_(\n",
    "                embed_sum, alpha=1 - self.ema_decay\n",
    "            )\n",
    "            \n",
    "            # Normalize\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.epsilon)\n",
    "                / (n + self.codebook_size * self.epsilon) * n\n",
    "            )\n",
    "            self.embedding.data.copy_(self.embed_avg / cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Commitment loss\n",
    "        commitment_loss = F.mse_loss(z_flat, z_q_flat.detach())\n",
    "        loss = self.commitment_weight * commitment_loss\n",
    "        \n",
    "        # Straight-through estimator\n",
    "        z_q_flat = z_flat + (z_q_flat - z_flat).detach()\n",
    "        \n",
    "        # Reshape back\n",
    "        z_q = z_q_flat.view(B, T, D).permute(0, 2, 1)\n",
    "        indices = indices.view(B, T)\n",
    "        \n",
    "        return z_q, loss, indices\n",
    "    \n",
    "    def decode(self, indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode indices to vectors.\"\"\"\n",
    "        B, T = indices.shape\n",
    "        z_q = self.embedding[indices.view(-1)].view(B, T, -1)\n",
    "        return z_q.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "# Test VQ\n",
    "vq = VectorQuantizer(codebook_size=1024, codebook_dim=128)\n",
    "z = torch.randn(2, 128, 50)  # 2 batch, 128 dim, 50 frames\n",
    "z_q, loss, indices = vq(z)\n",
    "print(f\"VQ: {z.shape} -> {z_q.shape}\")\n",
    "print(f\"Indices: {indices.shape}\")\n",
    "print(f\"Commitment loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Residual Vector Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualVQ(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Vector Quantization.\n",
    "    \n",
    "    Applies multiple VQ layers to the residual.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_quantizers: int = 8,\n",
    "        codebook_size: int = 1024,\n",
    "        codebook_dim: int = 128,\n",
    "        commitment_weight: float = 0.25,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_quantizers = num_quantizers\n",
    "        \n",
    "        self.quantizers = nn.ModuleList([\n",
    "            VectorQuantizer(\n",
    "                codebook_size=codebook_size,\n",
    "                codebook_dim=codebook_dim,\n",
    "                commitment_weight=commitment_weight,\n",
    "            )\n",
    "            for _ in range(num_quantizers)\n",
    "        ])\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        z: torch.Tensor,\n",
    "        num_quantizers: Optional[int] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: encoder output\n",
    "            num_quantizers: number of levels to use (for RVQ dropout)\n",
    "        \"\"\"\n",
    "        if num_quantizers is None:\n",
    "            num_quantizers = self.num_quantizers\n",
    "        \n",
    "        z_q = torch.zeros_like(z)\n",
    "        residual = z\n",
    "        total_loss = 0\n",
    "        all_indices = []\n",
    "        \n",
    "        for i in range(num_quantizers):\n",
    "            quantized, loss, indices = self.quantizers[i](residual)\n",
    "            \n",
    "            z_q = z_q + quantized\n",
    "            residual = residual - quantized\n",
    "            total_loss = total_loss + loss\n",
    "            all_indices.append(indices)\n",
    "        \n",
    "        return z_q, total_loss, all_indices\n",
    "    \n",
    "    def decode(self, indices_list: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Decode from list of indices.\"\"\"\n",
    "        z_q = 0\n",
    "        for i, indices in enumerate(indices_list):\n",
    "            z_q = z_q + self.quantizers[i].decode(indices)\n",
    "        return z_q\n",
    "\n",
    "\n",
    "# Test RVQ\n",
    "rvq = ResidualVQ(num_quantizers=8, codebook_size=1024, codebook_dim=128)\n",
    "z = torch.randn(2, 128, 50)\n",
    "z_q, loss, indices = rvq(z)\n",
    "print(f\"RVQ: {z.shape} -> {z_q.shape}\")\n",
    "print(f\"Number of codebook levels: {len(indices)}\")\n",
    "print(f\"Total loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compute reconstruction error at each level\n",
    "print(\"\\nReconstruction error by level:\")\n",
    "for n in [1, 2, 4, 8]:\n",
    "    z_q_n, _, _ = rvq(z, num_quantizers=n)\n",
    "    mse = F.mse_loss(z, z_q_n).item()\n",
    "    print(f\"  {n} levels: MSE = {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio decoder that reconstructs waveform from latent.\n",
    "    \n",
    "    Mirror of encoder with transposed convolutions for upsampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 32,\n",
    "        latent_dim: int = 128,\n",
    "        strides: List[int] = [8, 5, 4, 2],  # Reverse of encoder\n",
    "        num_residual: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Calculate channel progression\n",
    "        channel_mult = [min(2**i, 16) for i in range(len(strides), 0, -1)]\n",
    "        in_channels = [channels * m for m in channel_mult]\n",
    "        \n",
    "        # Initial conv from latent\n",
    "        layers.append(CausalConv1d(latent_dim, in_channels[0], kernel_size=7))\n",
    "        \n",
    "        # Initial residual blocks\n",
    "        for j in range(num_residual):\n",
    "            layers.append(ResidualBlock(in_channels[0], dilation=3**j))\n",
    "        \n",
    "        # Upsample blocks\n",
    "        for i, stride in enumerate(strides):\n",
    "            in_ch = in_channels[i]\n",
    "            out_ch = in_channels[i + 1] if i + 1 < len(in_channels) else channels\n",
    "            \n",
    "            # Transposed conv for upsampling\n",
    "            layers.append(nn.ELU())\n",
    "            layers.append(CausalConvTranspose1d(in_ch, out_ch, kernel_size=stride*2, stride=stride))\n",
    "            \n",
    "            # Residual blocks\n",
    "            for j in range(num_residual):\n",
    "                layers.append(ResidualBlock(out_ch, dilation=3**j))\n",
    "        \n",
    "        # Final conv to waveform\n",
    "        layers.append(nn.ELU())\n",
    "        layers.append(CausalConv1d(channels, 1, kernel_size=7))\n",
    "        layers.append(nn.Tanh())  # Output in [-1, 1]\n",
    "        \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"z: (batch, latent_dim, frames) -> (batch, 1, samples)\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "# Test decoder\n",
    "decoder = Decoder(channels=32, latent_dim=128)\n",
    "z = torch.randn(1, 128, 50)  # 50 frames\n",
    "x_hat = decoder(z)\n",
    "print(f\"Decoder: {z.shape} -> {x_hat.shape}\")\n",
    "print(f\"Upsampling: {z.shape[2]} -> {x_hat.shape[2]} ({x_hat.shape[2]//z.shape[2]}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAudioCodec(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete neural audio codec with encoder, RVQ, and decoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int = 16000,\n",
    "        channels: int = 32,\n",
    "        latent_dim: int = 128,\n",
    "        strides: List[int] = [2, 4, 5, 8],\n",
    "        num_quantizers: int = 8,\n",
    "        codebook_size: int = 1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            channels=channels,\n",
    "            latent_dim=latent_dim,\n",
    "            strides=strides,\n",
    "        )\n",
    "        \n",
    "        self.quantizer = ResidualVQ(\n",
    "            num_quantizers=num_quantizers,\n",
    "            codebook_size=codebook_size,\n",
    "            codebook_dim=latent_dim,\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            channels=channels,\n",
    "            latent_dim=latent_dim,\n",
    "            strides=strides[::-1],  # Reverse strides for decoder\n",
    "        )\n",
    "        \n",
    "        self.total_stride = np.prod(strides)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        \"\"\"Forward pass: encode, quantize, decode.\"\"\"\n",
    "        # Encode\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # Quantize\n",
    "        z_q, vq_loss, indices = self.quantizer(z)\n",
    "        \n",
    "        # Decode\n",
    "        x_hat = self.decoder(z_q)\n",
    "        \n",
    "        # Match lengths\n",
    "        min_len = min(x.shape[-1], x_hat.shape[-1])\n",
    "        x = x[..., :min_len]\n",
    "        x_hat = x_hat[..., :min_len]\n",
    "        \n",
    "        return x_hat, vq_loss, indices\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"Encode audio to tokens.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        _, _, indices = self.quantizer(z)\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Decode tokens to audio.\"\"\"\n",
    "        z_q = self.quantizer.decode(indices)\n",
    "        return self.decoder(z_q)\n",
    "\n",
    "\n",
    "# Test complete codec\n",
    "codec = NeuralAudioCodec(\n",
    "    sample_rate=16000,\n",
    "    channels=32,\n",
    "    latent_dim=128,\n",
    "    num_quantizers=8,\n",
    "    codebook_size=1024,\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1, 16000)  # 1 second @ 16kHz\n",
    "x_hat, vq_loss, indices = codec(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {x_hat.shape}\")\n",
    "print(f\"VQ Loss: {vq_loss.item():.4f}\")\n",
    "print(f\"Tokens per second: {len(indices)} levels Ã— {indices[0].shape[1]} frames = {len(indices) * indices[0].shape[1]} tokens\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in codec.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleSpectralLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale spectral loss (L1 + L2 on spectrograms).\n",
    "    \n",
    "    Computes loss at multiple STFT resolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ffts: List[int] = [512, 1024, 2048],\n",
    "        hop_lengths: List[int] = [128, 256, 512],\n",
    "        alpha: float = 1.0,  # L1 weight\n",
    "        beta: float = 1.0,   # L2 weight\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_ffts = n_ffts\n",
    "        self.hop_lengths = hop_lengths\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, x_hat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute multi-scale spectral loss.\"\"\"\n",
    "        loss = 0\n",
    "        \n",
    "        for n_fft, hop_length in zip(self.n_ffts, self.hop_lengths):\n",
    "            # Compute spectrograms\n",
    "            x_spec = torch.stft(\n",
    "                x.squeeze(1), n_fft=n_fft, hop_length=hop_length,\n",
    "                window=torch.hann_window(n_fft, device=x.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            x_hat_spec = torch.stft(\n",
    "                x_hat.squeeze(1), n_fft=n_fft, hop_length=hop_length,\n",
    "                window=torch.hann_window(n_fft, device=x_hat.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            \n",
    "            # Magnitude\n",
    "            x_mag = x_spec.abs()\n",
    "            x_hat_mag = x_hat_spec.abs()\n",
    "            \n",
    "            # L1 and L2 loss\n",
    "            loss += self.alpha * F.l1_loss(x_hat_mag, x_mag)\n",
    "            loss += self.beta * F.mse_loss(x_hat_mag, x_mag)\n",
    "        \n",
    "        return loss / len(self.n_ffts)\n",
    "\n",
    "\n",
    "# Test spectral loss\n",
    "spec_loss = MultiScaleSpectralLoss()\n",
    "x = torch.randn(2, 1, 16000)\n",
    "x_hat = torch.randn(2, 1, 16000)\n",
    "loss = spec_loss(x, x_hat)\n",
    "print(f\"Spectral loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_codec(\n",
    "    codec: NeuralAudioCodec,\n",
    "    num_steps: int = 100,\n",
    "    batch_size: int = 4,\n",
    "    audio_length: int = 16000,\n",
    "    lr: float = 3e-4,\n",
    "    device: str = 'cuda',\n",
    "):\n",
    "    \"\"\"Simple training loop for demonstration.\"\"\"\n",
    "    \n",
    "    codec = codec.to(device)\n",
    "    optimizer = torch.optim.Adam(codec.parameters(), lr=lr)\n",
    "    spec_loss_fn = MultiScaleSpectralLoss().to(device)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(\"Training codec...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Generate random audio (replace with real data)\n",
    "        x = torch.randn(batch_size, 1, audio_length, device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        x_hat, vq_loss, _ = codec(x)\n",
    "        \n",
    "        # Compute losses\n",
    "        recon_loss = spec_loss_fn(x, x_hat)\n",
    "        total_loss = recon_loss + vq_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(total_loss.item())\n",
    "        \n",
    "        if (step + 1) % 20 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Step {step+1}/{num_steps} | Loss: {total_loss.item():.4f} | \"\n",
    "                  f\"Recon: {recon_loss.item():.4f} | VQ: {vq_loss.item():.4f} | \"\n",
    "                  f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Run training (short demo)\n",
    "if torch.cuda.is_available():\n",
    "    codec = NeuralAudioCodec()\n",
    "    losses = train_codec(codec, num_steps=60, batch_size=4)\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"CUDA not available, skipping training demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_codec(codec: NeuralAudioCodec, audio_seconds: float = 10.0,\n",
    "                  num_runs: int = 50):\n",
    "    \"\"\"Profile encode/decode latency and throughput.\"\"\"\n",
    "    \n",
    "    device = next(codec.parameters()).device\n",
    "    audio = torch.randn(1, 1, int(codec.sample_rate * audio_seconds), device=device)\n",
    "    \n",
    "    codec.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            tokens = codec.encode(audio)\n",
    "            _ = codec.decode(tokens)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Profile encoding\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            tokens = codec.encode(audio)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    encode_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "    \n",
    "    # Profile decoding\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = codec.decode(tokens)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    decode_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rtf_encode = encode_time / 1000 / audio_seconds\n",
    "    rtf_decode = decode_time / 1000 / audio_seconds\n",
    "    \n",
    "    print(f\"\\nCodec Profiling ({audio_seconds}s audio):\")\n",
    "    print(f\"  Encode: {encode_time:.2f}ms (RTF: {rtf_encode:.4f})\")\n",
    "    print(f\"  Decode: {decode_time:.2f}ms (RTF: {rtf_decode:.4f})\")\n",
    "    print(f\"  Total RTF: {rtf_encode + rtf_decode:.4f}\")\n",
    "    print(f\"  {1/(rtf_encode + rtf_decode):.0f}x faster than real-time\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    profile_codec(codec, audio_seconds=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "1. **Add a discriminator** for adversarial training (like EnCodec)\n",
    "2. **Implement RVQ dropout** for variable bitrate\n",
    "3. **Add mel spectrogram loss** for better perceptual quality\n",
    "4. **Train on real audio** from LibriSpeech or similar\n",
    "5. **Implement streaming** encode/decode for real-time use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Neural codecs = Encoder + VQ + Decoder\n",
    "   - Encoder compresses waveform\n",
    "   - VQ discretizes for LLM compatibility\n",
    "   - Decoder reconstructs waveform\n",
    "\n",
    "2. RVQ enables high-fidelity with small codebooks\n",
    "   - Each level refines the residual\n",
    "   - Variable bitrate by using fewer levels\n",
    "\n",
    "3. EMA codebook updates are crucial\n",
    "   - More stable than gradient descent\n",
    "   - Avoids codebook collapse\n",
    "\n",
    "4. Spectral loss improves quality\n",
    "   - Multi-scale for different frequencies\n",
    "   - Perceptually meaningful\n",
    "\n",
    "5. Real systems add discriminators\n",
    "   - Adversarial training for sharper output\n",
    "   - Multi-scale and multi-period discriminators\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
