{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Audio LLM Inference Guide\n",
    "\n",
    "Hands-on guide for using audio language models including Moshi, EnCodec, and Whisper integration.\n",
    "\n",
    "**Covers:**\n",
    "- Audio tokenization with neural codecs\n",
    "- Semantic token extraction\n",
    "- LLM inference with audio tokens\n",
    "- Multi-stream processing\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audio Tokenization with EnCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from encodec import EncodecModel\n",
    "    from encodec.utils import convert_audio\n",
    "    \n",
    "    # Load EnCodec\n",
    "    encodec = EncodecModel.encodec_model_24khz()\n",
    "    encodec.set_target_bandwidth(6.0)  # 6 kbps\n",
    "    encodec = encodec.cuda() if torch.cuda.is_available() else encodec\n",
    "    encodec.eval()\n",
    "    \n",
    "    print(\"✓ EnCodec loaded\")\n",
    "    ENCODEC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"EnCodec not available. Install: pip install encodec\")\n",
    "    ENCODEC_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENCODEC_AVAILABLE:\n",
    "    # Generate test audio\n",
    "    test_audio = torch.randn(1, 1, 24000 * 5).to(encodec.device)  # 5 seconds\n",
    "    \n",
    "    # Encode to tokens\n",
    "    with torch.no_grad():\n",
    "        encoded_frames = encodec.encode(test_audio)\n",
    "    \n",
    "    # Extract codes\n",
    "    codes = [frame[0] for frame in encoded_frames]\n",
    "    codes_tensor = torch.cat(codes, dim=-1)\n",
    "    \n",
    "    print(f\"Audio shape: {test_audio.shape}\")\n",
    "    print(f\"Encoded codes shape: {codes_tensor.shape}\")\n",
    "    print(f\"Frame rate: {codes_tensor.shape[-1] / 5:.1f} Hz\")\n",
    "    print(f\"Tokens per second: {codes_tensor.shape[1] * codes_tensor.shape[-1] / 5:.0f}\")\n",
    "    \n",
    "    # Decode back\n",
    "    with torch.no_grad():\n",
    "        reconstructed = encodec.decode(encoded_frames)\n",
    "    \n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "    \n",
    "    # Measure reconstruction quality\n",
    "    mse = torch.nn.functional.mse_loss(test_audio, reconstructed)\n",
    "    print(f\"Reconstruction MSE: {mse.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Token Extraction with WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import WavLMModel, Wav2Vec2Processor\n",
    "    \n",
    "    # Load WavLM\n",
    "    wavlm = WavLMModel.from_pretrained(\"microsoft/wavlm-base\")\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"microsoft/wavlm-base\")\n",
    "    wavlm = wavlm.cuda() if torch.cuda.is_available() else wavlm\n",
    "    wavlm.eval()\n",
    "    \n",
    "    print(\"✓ WavLM loaded\")\n",
    "    WAVLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WavLM not available. Install: pip install transformers\")\n",
    "    WAVLM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WAVLM_AVAILABLE:\n",
    "    # Generate test audio (16kHz for WavLM)\n",
    "    test_audio_16k = torch.randn(1, 16000 * 5).to(wavlm.device)\n",
    "    \n",
    "    # Extract semantic features\n",
    "    with torch.no_grad():\n",
    "        outputs = wavlm(test_audio_16k, output_hidden_states=True)\n",
    "    \n",
    "    # Get features from layer 7 (good for semantic content)\n",
    "    semantic_features = outputs.hidden_states[7]\n",
    "    \n",
    "    print(f\"Audio shape: {test_audio_16k.shape}\")\n",
    "    print(f\"Semantic features shape: {semantic_features.shape}\")\n",
    "    print(f\"Feature rate: {semantic_features.shape[1] / 5:.1f} Hz\")\n",
    "    \n",
    "    # In practice, quantize these features with k-means\n",
    "    # to get discrete semantic tokens (like Mimi does)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Stream Token Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStreamTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenize audio for multi-stream LLM (Moshi-style).\n",
    "    \n",
    "    Streams:\n",
    "    - User audio (8 RVQ levels per frame)\n",
    "    - System audio (8 RVQ levels per frame)\n",
    "    - Text (1 token per ~4 frames)\n",
    "    \"\"\"\n",
    "    def __init__(self, codec, text_tokenizer, frame_rate=12.5):\n",
    "        self.codec = codec\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.frame_rate = frame_rate\n",
    "        self.text_rate = frame_rate / 4  # Text every 4 audio frames\n",
    "    \n",
    "    def tokenize(self, user_audio, system_audio, text):\n",
    "        \"\"\"\n",
    "        Tokenize all streams.\n",
    "        \n",
    "        Returns:\n",
    "            Interleaved token sequence for LLM\n",
    "        \"\"\"\n",
    "        # Tokenize audio\n",
    "        with torch.no_grad():\n",
    "            user_tokens = self.codec.encode(user_audio)\n",
    "            system_tokens = self.codec.encode(system_audio)\n",
    "        \n",
    "        # Tokenize text\n",
    "        text_tokens = self.text_tokenizer(text)\n",
    "        \n",
    "        # Interleave streams\n",
    "        num_frames = user_tokens.shape[-1]\n",
    "        interleaved = []\n",
    "        \n",
    "        for t in range(num_frames):\n",
    "            # User audio (8 tokens)\n",
    "            interleaved.extend(user_tokens[:, :, t].flatten().tolist())\n",
    "            \n",
    "            # System audio (8 tokens)\n",
    "            interleaved.extend(system_tokens[:, :, t].flatten().tolist())\n",
    "            \n",
    "            # Text (every 4 frames)\n",
    "            if t % 4 == 0 and t // 4 < len(text_tokens):\n",
    "                interleaved.append(text_tokens[t // 4])\n",
    "        \n",
    "        return torch.tensor(interleaved)\n",
    "\n",
    "\n",
    "# Demo\n",
    "if ENCODEC_AVAILABLE:\n",
    "    tokenizer = MultiStreamTokenizer(encodec, lambda x: [1, 2, 3])  # Dummy text tokenizer\n",
    "    \n",
    "    user_audio = torch.randn(1, 1, 24000).to(encodec.device)\n",
    "    system_audio = torch.randn(1, 1, 24000).to(encodec.device)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(user_audio, system_audio, \"test\")\n",
    "    print(f\"\\nInterleaved tokens: {tokens.shape}\")\n",
    "    print(f\"Tokens for 1 second: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_audio_tokenization(codec, audio_seconds=10.0, num_runs=50):\n",
    "    \"\"\"\n",
    "    Profile encode/decode latency.\n",
    "    \"\"\"\n",
    "    device = next(codec.parameters()).device\n",
    "    audio = torch.randn(1, 1, int(24000 * audio_seconds), device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            frames = codec.encode(audio)\n",
    "            _ = codec.decode(frames)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Profile encoding\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            frames = codec.encode(audio)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    encode_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "    \n",
    "    # Profile decoding\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = codec.decode(frames)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    decode_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "    \n",
    "    print(f\"\\nCodec Profiling ({audio_seconds}s audio):\")\n",
    "    print(f\"  Encode: {encode_time:.2f}ms (RTF: {encode_time/1000/audio_seconds:.4f})\")\n",
    "    print(f\"  Decode: {decode_time:.2f}ms (RTF: {decode_time/1000/audio_seconds:.4f})\")\n",
    "    print(f\"  Total RTF: {(encode_time+decode_time)/1000/audio_seconds:.4f}\")\n",
    "    print(f\"  Throughput: {1/((encode_time+decode_time)/1000/audio_seconds):.0f}x real-time\")\n",
    "\n",
    "\n",
    "if ENCODEC_AVAILABLE and torch.cuda.is_available():\n",
    "    profile_audio_tokenization(encodec, audio_seconds=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "1. **Neural codecs enable LLM compatibility** - Discrete tokens from continuous audio\n",
    "2. **Semantic tokens improve understanding** - WavLM features capture meaning\n",
    "3. **Multi-stream processing** - Handle user/system audio + text simultaneously\n",
    "4. **Profiling is critical** - Ensure real-time performance\n",
    "5. **Token rate matters** - Lower rate = longer context for LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
