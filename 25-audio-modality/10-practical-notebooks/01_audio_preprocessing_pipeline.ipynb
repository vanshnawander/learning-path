{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Preprocessing Pipeline\n",
    "\n",
    "Complete walkthrough of audio preprocessing for ML, with profiling at every step.\n",
    "\n",
    "**Topics covered:**\n",
    "- Audio loading and format handling\n",
    "- Resampling strategies\n",
    "- Spectrogram extraction\n",
    "- Mel spectrogram computation\n",
    "- Data augmentation\n",
    "- Batch processing optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install torch torchaudio librosa soundfile matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"torchaudio: {torchaudio.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audio Loading Comparison\n",
    "\n",
    "Compare different audio loading backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_audio(duration_sec: float, sample_rate: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"Generate synthetic speech-like audio for testing.\"\"\"\n",
    "    t = torch.linspace(0, duration_sec, int(duration_sec * sample_rate))\n",
    "    \n",
    "    # Multi-frequency signal (speech-like)\n",
    "    f0 = 150 + 50 * torch.sin(2 * np.pi * 0.5 * t)  # Varying pitch\n",
    "    audio = (\n",
    "        0.5 * torch.sin(2 * np.pi * f0 * t) +\n",
    "        0.3 * torch.sin(2 * np.pi * 2 * f0 * t) +\n",
    "        0.1 * torch.sin(2 * np.pi * 3 * f0 * t) +\n",
    "        0.05 * torch.randn_like(t)\n",
    "    )\n",
    "    return audio.unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "# Generate test audio\n",
    "test_audio = generate_test_audio(5.0, 16000)\n",
    "print(f\"Test audio shape: {test_audio.shape}\")\n",
    "print(f\"Duration: {test_audio.shape[1] / 16000:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test audio for loading experiments\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "wav_path = os.path.join(temp_dir, \"test.wav\")\n",
    "torchaudio.save(wav_path, test_audio, 16000)\n",
    "print(f\"Saved to: {wav_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_loading(path: str, n_runs: int = 10):\n",
    "    \"\"\"Benchmark different audio loading methods.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # torchaudio\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        audio, sr = torchaudio.load(path)\n",
    "    results['torchaudio'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    # librosa (if available)\n",
    "    try:\n",
    "        import librosa\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            audio, sr = librosa.load(path, sr=None)\n",
    "        results['librosa'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    except ImportError:\n",
    "        results['librosa'] = None\n",
    "    \n",
    "    # soundfile (if available)\n",
    "    try:\n",
    "        import soundfile as sf\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            audio, sr = sf.read(path)\n",
    "        results['soundfile'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    except ImportError:\n",
    "        results['soundfile'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "loading_results = benchmark_loading(wav_path)\n",
    "print(\"\\nAudio Loading Benchmark (ms):\")\n",
    "for method, time_ms in loading_results.items():\n",
    "    if time_ms is not None:\n",
    "        print(f\"  {method}: {time_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resampling\n",
    "\n",
    "Resampling is expensive - always pre-resample your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_resampling(audio: torch.Tensor, orig_sr: int, target_sr: int, n_runs: int = 10):\n",
    "    \"\"\"Benchmark resampling methods.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # torchaudio functional\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        resampled = torchaudio.functional.resample(audio, orig_sr, target_sr)\n",
    "    results['torchaudio_functional'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    # torchaudio transform (reusable kernel)\n",
    "    resampler = torchaudio.transforms.Resample(orig_sr, target_sr)\n",
    "    # Warmup\n",
    "    _ = resampler(audio)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        resampled = resampler(audio)\n",
    "    results['torchaudio_transform'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    # GPU resampling (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        audio_gpu = audio.cuda()\n",
    "        resampler_gpu = torchaudio.transforms.Resample(orig_sr, target_sr).cuda()\n",
    "        # Warmup\n",
    "        _ = resampler_gpu(audio_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            resampled = resampler_gpu(audio_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        results['torchaudio_gpu'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate 44.1kHz audio\n",
    "audio_44k = generate_test_audio(10.0, 44100)\n",
    "print(f\"Original: {audio_44k.shape[1]} samples @ 44100 Hz\")\n",
    "\n",
    "resample_results = benchmark_resampling(audio_44k, 44100, 16000)\n",
    "print(\"\\nResampling Benchmark (44.1kHz → 16kHz, 10s audio):\")\n",
    "for method, time_ms in resample_results.items():\n",
    "    print(f\"  {method}: {time_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spectrogram Extraction\n",
    "\n",
    "STFT and mel spectrogram computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    \"\"\"Whisper-style audio feature extractor.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int = 16000,\n",
    "        n_fft: int = 400,\n",
    "        hop_length: int = 160,\n",
    "        n_mels: int = 80,\n",
    "        device: str = \"cpu\"\n",
    "    ):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.device = device\n",
    "        \n",
    "        # Create mel spectrogram transform\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            norm=\"slaney\",\n",
    "            mel_scale=\"slaney\"\n",
    "        ).to(device)\n",
    "    \n",
    "    def extract(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract log mel spectrogram features.\"\"\"\n",
    "        audio = audio.to(self.device)\n",
    "        \n",
    "        # Compute mel spectrogram\n",
    "        mel_spec = self.mel_transform(audio)\n",
    "        \n",
    "        # Log compression\n",
    "        log_mel = torch.log(mel_spec.clamp(min=1e-10))\n",
    "        \n",
    "        return log_mel\n",
    "\n",
    "# Create extractor\n",
    "extractor = AudioFeatureExtractor()\n",
    "\n",
    "# Test\n",
    "features = extractor.extract(test_audio)\n",
    "print(f\"Input audio: {test_audio.shape}\")\n",
    "print(f\"Output features: {features.shape}\")\n",
    "print(f\"Feature rate: {features.shape[-1] / 5.0:.1f} frames/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_feature_extraction(audio: torch.Tensor, n_runs: int = 50):\n",
    "    \"\"\"Benchmark feature extraction.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # CPU extraction\n",
    "    extractor_cpu = AudioFeatureExtractor(device=\"cpu\")\n",
    "    # Warmup\n",
    "    _ = extractor_cpu.extract(audio)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_runs):\n",
    "        _ = extractor_cpu.extract(audio)\n",
    "    results['cpu'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    # GPU extraction (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        extractor_gpu = AudioFeatureExtractor(device=\"cuda\")\n",
    "        audio_gpu = audio.cuda()\n",
    "        # Warmup\n",
    "        _ = extractor_gpu.extract(audio_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(n_runs):\n",
    "            _ = extractor_gpu.extract(audio_gpu)\n",
    "        torch.cuda.synchronize()\n",
    "        results['gpu'] = (time.perf_counter() - start) / n_runs * 1000\n",
    "    \n",
    "    return results\n",
    "\n",
    "audio_10s = generate_test_audio(10.0, 16000)\n",
    "feat_results = benchmark_feature_extraction(audio_10s)\n",
    "print(\"\\nMel Spectrogram Extraction (10s audio):\")\n",
    "for device, time_ms in feat_results.items():\n",
    "    print(f\"  {device}: {time_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_and_spectrogram(audio: torch.Tensor, sample_rate: int = 16000):\n",
    "    \"\"\"Plot waveform and mel spectrogram.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Waveform\n",
    "    time_axis = torch.arange(audio.shape[1]) / sample_rate\n",
    "    axes[0].plot(time_axis.numpy(), audio[0].numpy())\n",
    "    axes[0].set_xlabel(\"Time (s)\")\n",
    "    axes[0].set_ylabel(\"Amplitude\")\n",
    "    axes[0].set_title(\"Waveform\")\n",
    "    \n",
    "    # Spectrogram\n",
    "    spec_transform = torchaudio.transforms.Spectrogram(n_fft=512, hop_length=160)\n",
    "    spec = spec_transform(audio)\n",
    "    spec_db = 10 * torch.log10(spec.clamp(min=1e-10))\n",
    "    axes[1].imshow(spec_db[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1].set_xlabel(\"Time frames\")\n",
    "    axes[1].set_ylabel(\"Frequency bins\")\n",
    "    axes[1].set_title(\"Spectrogram (dB)\")\n",
    "    \n",
    "    # Mel spectrogram\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate, n_fft=512, hop_length=160, n_mels=80\n",
    "    )\n",
    "    mel_spec = mel_transform(audio)\n",
    "    mel_db = 10 * torch.log10(mel_spec.clamp(min=1e-10))\n",
    "    axes[2].imshow(mel_db[0].numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[2].set_xlabel(\"Time frames\")\n",
    "    axes[2].set_ylabel(\"Mel bins\")\n",
    "    axes[2].set_title(\"Mel Spectrogram (dB)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_audio_and_spectrogram(test_audio, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmentation:\n",
    "    \"\"\"Common audio augmentations for training.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_noise(audio: torch.Tensor, snr_db: float = 20.0) -> torch.Tensor:\n",
    "        \"\"\"Add Gaussian noise at specified SNR.\"\"\"\n",
    "        signal_power = audio.pow(2).mean()\n",
    "        noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "        noise = torch.randn_like(audio) * noise_power.sqrt()\n",
    "        return audio + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_stretch(audio: torch.Tensor, rate: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"Time stretch without changing pitch (simplified).\"\"\"\n",
    "        # Use torchaudio's speed effect\n",
    "        effects = [[\"speed\", str(rate)]]\n",
    "        augmented, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
    "            audio, 16000, effects\n",
    "        )\n",
    "        return augmented\n",
    "    \n",
    "    @staticmethod\n",
    "    def pitch_shift(audio: torch.Tensor, semitones: float = 0.0) -> torch.Tensor:\n",
    "        \"\"\"Shift pitch by semitones.\"\"\"\n",
    "        effects = [[\"pitch\", str(semitones * 100)]]\n",
    "        augmented, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
    "            audio, 16000, effects\n",
    "        )\n",
    "        return augmented\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_crop(audio: torch.Tensor, length: int) -> torch.Tensor:\n",
    "        \"\"\"Random crop to fixed length.\"\"\"\n",
    "        if audio.shape[1] <= length:\n",
    "            # Pad if too short\n",
    "            pad_amount = length - audio.shape[1]\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad_amount))\n",
    "            return audio\n",
    "        \n",
    "        start = torch.randint(0, audio.shape[1] - length, (1,)).item()\n",
    "        return audio[:, start:start + length]\n",
    "\n",
    "# Demonstrate augmentations\n",
    "aug = AudioAugmentation()\n",
    "\n",
    "print(\"Original audio:\", test_audio.shape)\n",
    "print(\"With noise:\", aug.add_noise(test_audio, snr_db=20).shape)\n",
    "print(\"Random crop:\", aug.random_crop(test_audio, 48000).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPipeline:\n",
    "    \"\"\"\n",
    "    Complete audio preprocessing pipeline for training.\n",
    "    \n",
    "    Typical use for Whisper-like models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int = 16000,\n",
    "        n_mels: int = 80,\n",
    "        max_duration: float = 30.0,\n",
    "        augment: bool = True,\n",
    "        device: str = \"cpu\"\n",
    "    ):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.max_samples = int(max_duration * sample_rate)\n",
    "        self.augment = augment\n",
    "        self.device = device\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=400,\n",
    "            hop_length=160,\n",
    "            n_mels=n_mels\n",
    "        ).to(device)\n",
    "        \n",
    "        self.timing = {}\n",
    "    \n",
    "    def __call__(self, audio: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "        \"\"\"Process audio through full pipeline.\"\"\"\n",
    "        self.timing = {}\n",
    "        \n",
    "        # 1. Resample if needed\n",
    "        start = time.perf_counter()\n",
    "        if sr != self.sample_rate:\n",
    "            audio = torchaudio.functional.resample(audio, sr, self.sample_rate)\n",
    "        self.timing['resample'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # 2. Convert to mono if stereo\n",
    "        start = time.perf_counter()\n",
    "        if audio.shape[0] > 1:\n",
    "            audio = audio.mean(dim=0, keepdim=True)\n",
    "        self.timing['to_mono'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # 3. Pad/trim to max duration\n",
    "        start = time.perf_counter()\n",
    "        if audio.shape[1] > self.max_samples:\n",
    "            if self.augment:\n",
    "                # Random crop during training\n",
    "                start_idx = torch.randint(0, audio.shape[1] - self.max_samples, (1,)).item()\n",
    "                audio = audio[:, start_idx:start_idx + self.max_samples]\n",
    "            else:\n",
    "                # Trim from start during inference\n",
    "                audio = audio[:, :self.max_samples]\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            pad_amount = self.max_samples - audio.shape[1]\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad_amount))\n",
    "        self.timing['pad_trim'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # 4. Augmentation (training only)\n",
    "        start = time.perf_counter()\n",
    "        if self.augment:\n",
    "            # Add noise with probability 0.5\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                snr = torch.randint(15, 30, (1,)).item()\n",
    "                audio = AudioAugmentation.add_noise(audio, snr)\n",
    "        self.timing['augment'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # 5. Move to device\n",
    "        start = time.perf_counter()\n",
    "        audio = audio.to(self.device)\n",
    "        self.timing['to_device'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # 6. Extract mel spectrogram\n",
    "        start = time.perf_counter()\n",
    "        mel_spec = self.mel_transform(audio)\n",
    "        log_mel = torch.log(mel_spec.clamp(min=1e-10))\n",
    "        self.timing['mel_extract'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        # 7. Normalize\n",
    "        start = time.perf_counter()\n",
    "        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-8)\n",
    "        self.timing['normalize'] = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        return log_mel\n",
    "    \n",
    "    def print_timing(self):\n",
    "        \"\"\"Print timing breakdown.\"\"\"\n",
    "        total = sum(self.timing.values())\n",
    "        print(\"\\nPipeline Timing:\")\n",
    "        for step, time_ms in self.timing.items():\n",
    "            pct = 100 * time_ms / total if total > 0 else 0\n",
    "            print(f\"  {step:<15}: {time_ms:>8.3f} ms ({pct:>5.1f}%)\")\n",
    "        print(f\"  {'TOTAL':<15}: {total:>8.3f} ms\")\n",
    "\n",
    "# Test pipeline\n",
    "pipeline = AudioPipeline(augment=True, device=\"cpu\")\n",
    "features = pipeline(test_audio, 16000)\n",
    "print(f\"Input: {test_audio.shape}\")\n",
    "print(f\"Output: {features.shape}\")\n",
    "pipeline.print_timing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory(duration_sec: float = 30.0, sample_rate: int = 16000):\n",
    "    \"\"\"Analyze memory usage for different representations.\"\"\"\n",
    "    print(f\"\\nMemory Analysis for {duration_sec}s audio @ {sample_rate} Hz:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    num_samples = int(duration_sec * sample_rate)\n",
    "    \n",
    "    # Raw audio (int16)\n",
    "    raw_int16 = num_samples * 2  # 2 bytes per sample\n",
    "    print(f\"Raw audio (int16):     {raw_int16 / 1024:.2f} KB\")\n",
    "    \n",
    "    # Raw audio (float32)\n",
    "    raw_float32 = num_samples * 4  # 4 bytes per sample\n",
    "    print(f\"Raw audio (float32):   {raw_float32 / 1024:.2f} KB\")\n",
    "    \n",
    "    # Spectrogram\n",
    "    n_fft = 512\n",
    "    hop_length = 160\n",
    "    num_frames = (num_samples - n_fft) // hop_length + 1\n",
    "    num_bins = n_fft // 2 + 1\n",
    "    spec_size = num_frames * num_bins * 4  # float32\n",
    "    print(f\"Spectrogram:           {spec_size / 1024:.2f} KB ({num_frames} × {num_bins})\")\n",
    "    \n",
    "    # Mel spectrogram\n",
    "    n_mels = 80\n",
    "    mel_size = num_frames * n_mels * 4  # float32\n",
    "    print(f\"Mel spectrogram:       {mel_size / 1024:.2f} KB ({num_frames} × {n_mels})\")\n",
    "    \n",
    "    # Compression ratios\n",
    "    print(\"\\nCompression vs raw float32:\")\n",
    "    print(f\"  Mel spectrogram: {raw_float32 / mel_size:.1f}x\")\n",
    "\n",
    "analyze_memory(30.0, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "1. **Pre-resample your data** - Resampling is expensive, do it once during dataset preparation\n",
    "\n",
    "2. **Use GPU for feature extraction** - 10-20x speedup for mel spectrograms\n",
    "\n",
    "3. **Mel spectrograms are efficient** - Smaller than raw audio at 16kHz\n",
    "\n",
    "4. **Augmentation is cheap** - Noise addition and time stretching are fast\n",
    "\n",
    "5. **Batch processing helps** - Process multiple files together for better GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "print(\"Cleaned up temporary files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
