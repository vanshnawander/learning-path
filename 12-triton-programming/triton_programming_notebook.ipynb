{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Programming - Interactive Notebook\n",
    "\n",
    "Learn GPU programming with Triton - high-level, Pythonic, and fast!\n",
    "\n",
    "## Topics\n",
    "1. Your First Triton Kernel\n",
    "2. Memory Access Patterns\n",
    "3. Auto-tuning\n",
    "4. Fused Operations\n",
    "5. Softmax Implementation\n",
    "6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Triton: {triton.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling utility\n",
    "def profile_triton(func, warmup=25, iterations=100):\n",
    "    \"\"\"Profile a Triton/PyTorch function.\"\"\"\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(iterations):\n",
    "        func()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end) / iterations\n",
    "\n",
    "print(\"Profiling ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Your First Triton Kernel: Vector Addition\n",
    "\n",
    "The \"Hello World\" of GPU programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,      # Pointer to input x\n",
    "    y_ptr,      # Pointer to input y  \n",
    "    out_ptr,    # Pointer to output\n",
    "    n_elements, # Total elements\n",
    "    BLOCK_SIZE: tl.constexpr,  # Compile-time constant\n",
    "):\n",
    "    \"\"\"\n",
    "    Vector addition: out = x + y\n",
    "    \n",
    "    Key Triton concepts:\n",
    "    - tl.program_id(0): Which block am I? (like blockIdx.x in CUDA)\n",
    "    - tl.arange(0, BLOCK_SIZE): Range of offsets within block\n",
    "    - tl.load/tl.store: Memory operations with automatic coalescing\n",
    "    - mask: Handle boundary conditions\n",
    "    \"\"\"\n",
    "    # Which block (program) is this?\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    # Calculate offsets for this block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Mask for out-of-bounds elements\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Compute\n",
    "    out = x + y\n",
    "    \n",
    "    # Store result\n",
    "    tl.store(out_ptr + offsets, out, mask=mask)\n",
    "\n",
    "print(\"Kernel defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_add(x, y):\n",
    "    \"\"\"Wrapper function for the Triton kernel.\"\"\"\n",
    "    output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    \n",
    "    # Calculate grid size (number of blocks)\n",
    "    BLOCK_SIZE = 1024\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    \n",
    "    # Launch kernel\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test correctness\n",
    "size = 1_000_000\n",
    "x = torch.randn(size, device='cuda')\n",
    "y = torch.randn(size, device='cuda')\n",
    "\n",
    "out_triton = triton_add(x, y)\n",
    "out_torch = x + y\n",
    "\n",
    "print(f\"Correctness: {torch.allclose(out_triton, out_torch)}\")\n",
    "print(f\"Max difference: {torch.max(torch.abs(out_triton - out_torch)):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(f\"\\nPerformance ({size/1e6:.0f}M elements):\")\n",
    "print(f\"{'Method':<20} {'Time (ms)':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "time_triton = profile_triton(lambda: triton_add(x, y))\n",
    "time_torch = profile_triton(lambda: x + y)\n",
    "\n",
    "print(f\"{'Triton':<20} {time_triton:.4f}\")\n",
    "print(f\"{'PyTorch':<20} {time_torch:.4f}\")\n",
    "print(f\"\\nFor simple ops, PyTorch is already optimized.\")\n",
    "print(f\"Triton shines for CUSTOM fused operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fused Operations - Where Triton Shines\n",
    "\n",
    "Triton excels at fusing multiple operations into one kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def fused_gelu_kernel(\n",
    "    x_ptr, out_ptr, n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused GELU activation.\n",
    "    GELU(x) ≈ 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x³)))\n",
    "    \n",
    "    All operations fused = minimal memory traffic!\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Load\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Fused GELU computation (all in registers!)\n",
    "    x3 = x * x * x\n",
    "    inner = 0.7978845608 * (x + 0.044715 * x3)  # sqrt(2/pi)\n",
    "    tanh_inner = tl.libdevice.tanh(inner)\n",
    "    result = 0.5 * x * (1.0 + tanh_inner)\n",
    "    \n",
    "    # Store\n",
    "    tl.store(out_ptr + offsets, result, mask=mask)\n",
    "\n",
    "def triton_gelu(x):\n",
    "    out = torch.empty_like(x)\n",
    "    n = out.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)\n",
    "    fused_gelu_kernel[grid](x, out, n, BLOCK_SIZE=1024)\n",
    "    return out\n",
    "\n",
    "print(\"Fused GELU kernel defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "size = 16_000_000\n",
    "x = torch.randn(size, device='cuda')\n",
    "\n",
    "# Verify correctness\n",
    "out_triton = triton_gelu(x)\n",
    "out_torch = F.gelu(x)\n",
    "print(f\"Correctness: {torch.allclose(out_triton, out_torch, atol=1e-5)}\")\n",
    "\n",
    "# Manual unfused GELU\n",
    "def manual_gelu(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(0.7978845608 * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Benchmark\n",
    "time_manual = profile_triton(lambda: manual_gelu(x))\n",
    "time_pytorch = profile_triton(lambda: F.gelu(x))\n",
    "time_triton = profile_triton(lambda: triton_gelu(x))\n",
    "\n",
    "print(f\"\\nGELU Performance ({size/1e6:.0f}M elements):\")\n",
    "print(f\"{'Method':<25} {'Time (ms)':<15} {'Speedup'}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Manual (unfused)':<25} {time_manual:<15.3f} 1.0x\")\n",
    "print(f\"{'PyTorch F.gelu':<25} {time_pytorch:<15.3f} {time_manual/time_pytorch:.2f}x\")\n",
    "print(f\"{'Triton fused':<25} {time_triton:<15.3f} {time_manual/time_triton:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Auto-tuning\n",
    "\n",
    "Triton can automatically find optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE': 64}),\n",
    "        triton.Config({'BLOCK_SIZE': 128}),\n",
    "        triton.Config({'BLOCK_SIZE': 256}),\n",
    "        triton.Config({'BLOCK_SIZE': 512}),\n",
    "        triton.Config({'BLOCK_SIZE': 1024}),\n",
    "    ],\n",
    "    key=['n_elements'],  # Re-tune when size changes\n",
    ")\n",
    "@triton.jit\n",
    "def autotuned_add_kernel(\n",
    "    x_ptr, y_ptr, out_ptr, n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    tl.store(out_ptr + offsets, x + y, mask=mask)\n",
    "\n",
    "def autotuned_add(x, y):\n",
    "    out = torch.empty_like(x)\n",
    "    n = out.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)\n",
    "    autotuned_add_kernel[grid](x, y, out, n)\n",
    "    return out\n",
    "\n",
    "# Test auto-tuning at different sizes\n",
    "print(\"Auto-tuning in action:\")\n",
    "for size in [1024, 65536, 1048576, 16777216]:\n",
    "    x = torch.randn(size, device='cuda')\n",
    "    y = torch.randn(size, device='cuda')\n",
    "    \n",
    "    # First call triggers auto-tuning\n",
    "    _ = autotuned_add(x, y)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    time_ms = profile_triton(lambda: autotuned_add(x, y))\n",
    "    print(f\"  Size {size:>10}: {time_ms:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Softmax - A Complete Example\n",
    "\n",
    "Softmax shows reduction + element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr, output_ptr,\n",
    "    input_stride, output_stride,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fused softmax kernel - one row per program.\n",
    "    \n",
    "    softmax(x_i) = exp(x_i - max) / sum(exp(x_j - max))\n",
    "    \n",
    "    All in one kernel = minimal memory traffic!\n",
    "    \"\"\"\n",
    "    row_idx = tl.program_id(0)\n",
    "    \n",
    "    # Pointers to this row\n",
    "    row_start = input_ptr + row_idx * input_stride\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Load row (mask for valid columns)\n",
    "    mask = col_offsets < n_cols\n",
    "    row = tl.load(row_start + col_offsets, mask=mask, other=-float('inf'))\n",
    "    \n",
    "    # Compute softmax\n",
    "    row_max = tl.max(row, axis=0)\n",
    "    numerator = tl.exp(row - row_max)\n",
    "    numerator = tl.where(mask, numerator, 0.0)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_out = numerator / denominator\n",
    "    \n",
    "    # Store\n",
    "    out_start = output_ptr + row_idx * output_stride\n",
    "    tl.store(out_start + col_offsets, softmax_out, mask=mask)\n",
    "\n",
    "def triton_softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    BLOCK_SIZE = min(BLOCK_SIZE, 8192)\n",
    "    \n",
    "    out = torch.empty_like(x)\n",
    "    softmax_kernel[(n_rows,)](\n",
    "        x, out,\n",
    "        x.stride(0), out.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "print(\"Softmax kernel defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test softmax\n",
    "rows, cols = 1024, 1024\n",
    "x = torch.randn(rows, cols, device='cuda')\n",
    "\n",
    "out_triton = triton_softmax(x)\n",
    "out_torch = F.softmax(x, dim=-1)\n",
    "\n",
    "print(f\"Shape: {rows}x{cols}\")\n",
    "print(f\"Correctness: {torch.allclose(out_triton, out_torch, atol=1e-5)}\")\n",
    "\n",
    "# Benchmark at different sizes\n",
    "print(f\"\\nSoftmax Performance:\")\n",
    "print(f\"{'Shape':<18} {'PyTorch (ms)':<15} {'Triton (ms)':<15} {'Speedup'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for rows, cols in [(128, 128), (512, 512), (1024, 1024), (2048, 2048)]:\n",
    "    x = torch.randn(rows, cols, device='cuda')\n",
    "    \n",
    "    time_torch = profile_triton(lambda: F.softmax(x, dim=-1))\n",
    "    time_triton = profile_triton(lambda: triton_softmax(x))\n",
    "    \n",
    "    print(f\"{str((rows,cols)):<18} {time_torch:<15.4f} {time_triton:<15.4f} {time_torch/time_triton:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                   TRITON PROGRAMMING SUMMARY                     ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  TRITON KERNEL TEMPLATE:                                         ║\n",
    "║  ──────────────────────────────────────────────────────────────  ║\n",
    "║  @triton.jit                                                     ║\n",
    "║  def kernel(ptr, n, BLOCK: tl.constexpr):                        ║\n",
    "║      pid = tl.program_id(0)           # Block index              ║\n",
    "║      offs = pid * BLOCK + tl.arange(0, BLOCK)  # Offsets         ║\n",
    "║      mask = offs < n                  # Bounds check             ║\n",
    "║      data = tl.load(ptr + offs, mask=mask)  # Load               ║\n",
    "║      # ... compute ...                                           ║\n",
    "║      tl.store(ptr + offs, data, mask=mask)  # Store              ║\n",
    "║                                                                  ║\n",
    "║  KEY FUNCTIONS:                                                  ║\n",
    "║  ──────────────────────────────────────────────────────────────  ║\n",
    "║  • tl.program_id(axis)  - Block index                            ║\n",
    "║  • tl.arange(start,end) - Range of offsets                       ║\n",
    "║  • tl.load(ptr, mask)   - Load with masking                      ║\n",
    "║  • tl.store(ptr, val)   - Store with masking                     ║\n",
    "║  • tl.dot(a, b)         - Matrix multiply (Tensor Cores!)        ║\n",
    "║  • tl.sum/max/min       - Reductions                             ║\n",
    "║                                                                  ║\n",
    "║  WHEN TO USE TRITON:                                             ║\n",
    "║  ──────────────────────────────────────────────────────────────  ║\n",
    "║  ✓ Custom fused operations                                       ║\n",
    "║  ✓ Operations not in PyTorch                                     ║\n",
    "║  ✓ Memory-bound ops needing fusion                               ║\n",
    "║  ✓ Flash Attention variants                                      ║\n",
    "║  ✗ Simple ops (PyTorch is already optimized)                     ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
