{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Programming Fundamentals - Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on experiments with CUDA programming concepts.\n",
    "Each cell demonstrates a key concept with profiled measurements.\n",
    "\n",
    "## Topics\n",
    "1. Thread Hierarchy\n",
    "2. Memory Coalescing\n",
    "3. Kernel Fusion\n",
    "4. Memory-Bound vs Compute-Bound\n",
    "5. Synchronization\n",
    "6. Profiling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling utility with proper CUDA timing\n",
    "def profile_cuda(func, warmup=10, iterations=100, name=\"\"):\n",
    "    \"\"\"Profile CUDA function with proper synchronization.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0.0\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Time with CUDA events\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(iterations):\n",
    "        func()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return start.elapsed_time(end) / iterations\n",
    "\n",
    "print(\"Profiling utility ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CUDA Thread Hierarchy\n",
    "\n",
    "```\n",
    "Grid (all blocks)\n",
    "└── Block (group of threads, share memory)\n",
    "    └── Warp (32 threads, execute together)\n",
    "        └── Thread (individual execution unit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how work scales with size\n",
    "print(\"How PyTorch launches kernels (conceptually):\")\n",
    "print(f\"{'Size':<15} {'Est. Blocks':<15} {'Threads/Block':<15} {'Time (ms)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for size in [1024, 65536, 1048576, 16777216]:\n",
    "    x = torch.randn(size, device='cuda')\n",
    "    \n",
    "    time_ms = profile_cuda(lambda: x + 1.0, iterations=1000)\n",
    "    \n",
    "    # Estimate launch config (256 threads/block is common)\n",
    "    threads_per_block = 256\n",
    "    num_blocks = (size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    print(f\"{size:<15} {num_blocks:<15} {threads_per_block:<15} {time_ms:.4f}\")\n",
    "\n",
    "print(\"\\nKey insight: More elements = more blocks = more parallelism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Coalescing\n",
    "\n",
    "When threads in a warp access consecutive memory addresses, GPU combines into fewer transactions.\n",
    "\n",
    "**Good (coalesced):** Thread 0→addr 0, Thread 1→addr 1, Thread 2→addr 2...\n",
    "\n",
    "**Bad (strided):** Thread 0→addr 0, Thread 1→addr N, Thread 2→addr 2N..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesced vs strided access\n",
    "rows, cols = 4096, 4096\n",
    "x = torch.randn(rows, cols, device='cuda')\n",
    "\n",
    "print(f\"Matrix shape: {rows} x {cols}\")\n",
    "print(f\"\\n{'Access Pattern':<30} {'Time (ms)':<15} {'Bandwidth (GB/s)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "bytes_total = rows * cols * 4\n",
    "\n",
    "# Row-wise (coalesced - last dim is contiguous)\n",
    "time_row = profile_cuda(lambda: x.sum(dim=1))\n",
    "bw_row = bytes_total / (time_row / 1000) / 1e9\n",
    "print(f\"{'Row sum (coalesced)':<30} {time_row:<15.3f} {bw_row:.1f}\")\n",
    "\n",
    "# Column-wise (strided)\n",
    "time_col = profile_cuda(lambda: x.sum(dim=0))\n",
    "bw_col = bytes_total / (time_col / 1000) / 1e9\n",
    "print(f\"{'Column sum (strided)':<30} {time_col:<15.3f} {bw_col:.1f}\")\n",
    "\n",
    "print(f\"\\nStrided is {time_col/time_row:.1f}x slower!\")\n",
    "print(\"Always access memory along the contiguous (last) dimension!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel Fusion\n",
    "\n",
    "Fusing operations reduces memory traffic - intermediate results stay in registers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 16_000_000\n",
    "x = torch.randn(size, device='cuda')\n",
    "\n",
    "# Unfused: Each operation writes to memory\n",
    "def unfused():\n",
    "    a = x + 1\n",
    "    b = a * 2\n",
    "    c = b - 0.5\n",
    "    d = torch.relu(c)\n",
    "    return d\n",
    "\n",
    "# Partially fused (compiler may help)\n",
    "def partially_fused():\n",
    "    return torch.relu(x * 2 + 1.5)\n",
    "\n",
    "time_unfused = profile_cuda(unfused)\n",
    "time_fused = profile_cuda(partially_fused)\n",
    "\n",
    "print(f\"Element-wise operations ({size/1e6:.0f}M elements):\")\n",
    "print(f\"  Unfused (4 kernels): {time_unfused:.3f} ms\")\n",
    "print(f\"  Fused (1 kernel):    {time_fused:.3f} ms\")\n",
    "print(f\"  Speedup: {time_unfused/time_fused:.2f}x\")\n",
    "print(f\"\\nMemory traffic: Unfused ~8 passes, Fused ~2 passes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.compile fusion\n",
    "try:\n",
    "    compiled_unfused = torch.compile(unfused)\n",
    "    _ = compiled_unfused()  # Warmup\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    time_compiled = profile_cuda(compiled_unfused)\n",
    "    print(f\"torch.compile (auto-fused): {time_compiled:.3f} ms\")\n",
    "    print(f\"Speedup vs unfused: {time_unfused/time_compiled:.2f}x\")\n",
    "except Exception as e:\n",
    "    print(f\"torch.compile not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory-Bound vs Compute-Bound\n",
    "\n",
    "**Arithmetic Intensity** = FLOPs / Bytes moved\n",
    "\n",
    "- Low AI → Memory-bound (waiting for data)\n",
    "- High AI → Compute-bound (waiting for math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4096\n",
    "\n",
    "print(f\"{'Operation':<25} {'AI (FLOP/Byte)':<18} {'Type':<15} {'Time (ms)'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Vector add: 1 FLOP per 12 bytes\n",
    "a = torch.randn(N * N, device='cuda')\n",
    "b = torch.randn(N * N, device='cuda')\n",
    "time_add = profile_cuda(lambda: a + b)\n",
    "print(f\"{'a + b':<25} {1/12:<18.3f} {'MEMORY':<15} {time_add:.3f}\")\n",
    "\n",
    "# Softmax: ~10 FLOPs per 8 bytes\n",
    "x = torch.randn(N, N, device='cuda')\n",
    "time_soft = profile_cuda(lambda: F.softmax(x, dim=-1))\n",
    "print(f\"{'softmax':<25} {10/8:<18.3f} {'MEMORY':<15} {time_soft:.3f}\")\n",
    "\n",
    "# Matrix multiply: 2N FLOPs per element\n",
    "A = torch.randn(N, N, device='cuda')\n",
    "B = torch.randn(N, N, device='cuda')\n",
    "time_mm = profile_cuda(lambda: A @ B, iterations=50)\n",
    "print(f\"{'A @ B':<25} {2*N/12:<18.0f} {'COMPUTE':<15} {time_mm:.3f}\")\n",
    "\n",
    "print(f\"\\nMost element-wise ops are memory-bound!\")\n",
    "print(f\"Matrix multiply is compute-bound (uses Tensor Cores).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synchronization Importance\n",
    "\n",
    "CUDA operations are **asynchronous** - they return immediately!\n",
    "Must synchronize for correct timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10000, 10000, device='cuda')\n",
    "\n",
    "# WRONG: Without sync (measures only launch time)\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    y = x @ x\n",
    "no_sync_time = (time.perf_counter() - start) * 1000 / 10\n",
    "torch.cuda.synchronize()  # Complete the work\n",
    "\n",
    "# CORRECT: With sync (measures actual execution)\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    y = x @ x\n",
    "torch.cuda.synchronize()\n",
    "sync_time = (time.perf_counter() - start) * 1000 / 10\n",
    "\n",
    "print(f\"Without sync (WRONG): {no_sync_time:.2f} ms\")\n",
    "print(f\"With sync (CORRECT):  {sync_time:.2f} ms\")\n",
    "print(f\"\\nWithout sync is {sync_time/no_sync_time:.0f}x faster - but it's a lie!\")\n",
    "print(\"Always use CUDA events or synchronize for timing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mixed Precision\n",
    "\n",
    "FP16/BF16 uses Tensor Cores for massive speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4096\n",
    "A = torch.randn(N, N, device='cuda')\n",
    "B = torch.randn(N, N, device='cuda')\n",
    "\n",
    "# FP32\n",
    "time_fp32 = profile_cuda(lambda: A @ B, iterations=50)\n",
    "\n",
    "# FP16\n",
    "A_fp16 = A.half()\n",
    "B_fp16 = B.half()\n",
    "time_fp16 = profile_cuda(lambda: A_fp16 @ B_fp16, iterations=50)\n",
    "\n",
    "# Calculate TFLOPS\n",
    "flops = 2 * N * N * N\n",
    "tflops_fp32 = flops / (time_fp32 / 1000) / 1e12\n",
    "tflops_fp16 = flops / (time_fp16 / 1000) / 1e12\n",
    "\n",
    "print(f\"Matrix multiply {N}x{N}:\")\n",
    "print(f\"  FP32: {time_fp32:.3f} ms ({tflops_fp32:.1f} TFLOPS)\")\n",
    "print(f\"  FP16: {time_fp16:.3f} ms ({tflops_fp16:.1f} TFLOPS)\")\n",
    "print(f\"  Speedup: {time_fp32/time_fp16:.2f}x\")\n",
    "print(f\"\\nFP16 uses Tensor Cores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Flash Attention Benefit\n",
    "\n",
    "Flash Attention uses O(N) memory instead of O(N²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, heads, head_dim = 4, 8, 64\n",
    "\n",
    "print(f\"Attention comparison (batch={batch}, heads={heads}, head_dim={head_dim}):\")\n",
    "print(f\"\\n{'Seq Len':<12} {'Standard (ms)':<18} {'SDPA (ms)':<18} {'Speedup'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for seq_len in [256, 512, 1024, 2048]:\n",
    "    Q = torch.randn(batch, heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    K = torch.randn(batch, heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    V = torch.randn(batch, heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Standard attention\n",
    "    def standard():\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, V)\n",
    "    \n",
    "    # SDPA (uses Flash Attention)\n",
    "    def sdpa():\n",
    "        return F.scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    time_standard = profile_cuda(standard, iterations=50)\n",
    "    time_sdpa = profile_cuda(sdpa, iterations=50)\n",
    "    \n",
    "    print(f\"{seq_len:<12} {time_standard:<18.3f} {time_sdpa:<18.3f} {time_standard/time_sdpa:.2f}x\")\n",
    "\n",
    "print(f\"\\nFlash Attention: O(N) memory instead of O(N²)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key CUDA concepts demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                 CUDA FUNDAMENTALS SUMMARY                        ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║                                                                  ║\n",
    "║  1. THREAD HIERARCHY                                             ║\n",
    "║     Grid → Blocks → Warps → Threads                              ║\n",
    "║     More elements = more parallelism                             ║\n",
    "║                                                                  ║\n",
    "║  2. MEMORY COALESCING                                            ║\n",
    "║     Access consecutive addresses within warps                    ║\n",
    "║     Strided access is 2-10x slower!                              ║\n",
    "║                                                                  ║\n",
    "║  3. KERNEL FUSION                                                ║\n",
    "║     Fuse operations to reduce memory traffic                     ║\n",
    "║     torch.compile does this automatically                        ║\n",
    "║                                                                  ║\n",
    "║  4. MEMORY vs COMPUTE BOUND                                      ║\n",
    "║     Most ops are memory-bound                                    ║\n",
    "║     Matmul is compute-bound (Tensor Cores)                       ║\n",
    "║                                                                  ║\n",
    "║  5. SYNCHRONIZATION                                              ║\n",
    "║     Always sync for correct timing!                              ║\n",
    "║     Use CUDA events for best precision                           ║\n",
    "║                                                                  ║\n",
    "║  6. MIXED PRECISION                                              ║\n",
    "║     FP16/BF16 = Tensor Cores = massive speedup                   ║\n",
    "║                                                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
